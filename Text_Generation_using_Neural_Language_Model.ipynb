{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation using Neural Language Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0KeXkRa16v9C/aaZUXSfB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e807aef10ff46e69e481e407909c438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_309594052e8f4cc6bd56586f0ede2806",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1619c0818de640ec8cdf5d8f133a11d5",
              "IPY_MODEL_7413574837c24c2cb47046486eaafdeb"
            ]
          }
        },
        "309594052e8f4cc6bd56586f0ede2806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1619c0818de640ec8cdf5d8f133a11d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_988c6e3185c540bf93aee058a7917f09",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 64776,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 64776,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ebe8732fc2a34957a983365fba265a84"
          }
        },
        "7413574837c24c2cb47046486eaafdeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c1669e81f76249fab9ed0f3d542c187a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 64776/64776 [00:31&lt;00:00, 2029.81it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46cb1d8105484c2c929ffec1b22aa45e"
          }
        },
        "988c6e3185c540bf93aee058a7917f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ebe8732fc2a34957a983365fba265a84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1669e81f76249fab9ed0f3d542c187a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46cb1d8105484c2c929ffec1b22aa45e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS4FSzhvCcQm",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj4qQ2UyChm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import random\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb32jHATCodt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ea59a8a-2144-4587-8e9e-7954b6f65516"
      },
      "source": [
        "# reproducing same results\n",
        "SEED = 2019\n",
        "\n",
        "# torch\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f19ca501360>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWLJs5qNDNem",
        "colab_type": "text"
      },
      "source": [
        "# **2 Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdX-tiFRDQaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open text file and read in data\n",
        "with open(\"Dailog-dataset.dialogs_dataset\", \"rb\") as f:\n",
        "  dialogs = pickle.load(f)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfqiIL_EFEXI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c06e4130-39d7-4f7f-a059-c4c36b5f2e1b"
      },
      "source": [
        "# number of text sequences\n",
        "len(dialogs)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq50j_IKFTxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "c0c295ef-16de-48bc-914d-4dffd8bd8caf"
      },
      "source": [
        "# print 10 random dialogs\n",
        "random.sample(dialogs, 10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Hey Janice! I'm thinking of making a reservation at a seafood place for dinner tomorrow\",\n",
              " 'What time does the restuarant close?',\n",
              " 'and extra cinnamon',\n",
              " 'Terrific, thank you',\n",
              " 'Actually can you add green pepper for half of the pizza',\n",
              " 'Do I need to bring anything else',\n",
              " \"Think I'll go with regular\",\n",
              " 'Okay that is fine',\n",
              " 'How much will this cost',\n",
              " 'Sorry, I cannot make it that early']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhKaN4WaFbNz",
        "colab_type": "text"
      },
      "source": [
        "# **3 Preprocessing and Exploring Text Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3nRLnb0en99",
        "colab_type": "text"
      },
      "source": [
        "# **3.1 Text Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX8cE7ihFfIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text cleaning\n",
        "dialogs_clean = []\n",
        "\n",
        "for i in dialogs:\n",
        "  # remove everything except alphabets\n",
        "  i = re.sub(\"[^a-zA-Z' ]\", \"\", i)\n",
        "  # convert text to lowercase\n",
        "  i = i.lower()\n",
        "  # add cleaned text to the list\n",
        "  dialogs_clean.append(i)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBIleXF9fUkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "8b5639f5-97df-4961-eaf5-48dd4e8bbed9"
      },
      "source": [
        "random.sample(dialogs_clean, 10)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['they make it with skim milk',\n",
              " 'i said no romance movies',\n",
              " ' and can you get it gluten free',\n",
              " 'tonight if possible',\n",
              " 'hello pa can you get me to intelligent auto solutions',\n",
              " 'i prefer a booth',\n",
              " 'your very welcome',\n",
              " 'thats everything thank you ',\n",
              " 'any good specials online from dominos today',\n",
              " 'pizza hut the closest near me for pickup']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B7njUCufhWJ",
        "colab_type": "text"
      },
      "source": [
        "# **3.2 Finding Word Count**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQTSv2ChfjhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get list of all the words\n",
        "all_words = \" \".join(dialogs_clean).split()\n",
        "\n",
        "words_dict = {}\n",
        "\n",
        "# add word-count pair to the dictionary\n",
        "for word in all_words:   \n",
        "  # check if the word is already in dictionary \n",
        "  if word in words_dict:\n",
        "    # increment count of word by 1 \n",
        "    words_dict[word] = words_dict[word] + 1\n",
        "  else:\n",
        "    # add the word to dictionary with count 1 \n",
        "    words_dict[word] = 1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jKfxnI9f_JM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare a dataframe\n",
        "words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())})\n",
        "\n",
        "# sort words by their count in increasing order\n",
        "words_df = words_df.sort_values(by = ['count'])\n",
        "\n",
        "# reset dataframe index\n",
        "words_df.reset_index(inplace = True, drop=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q42tgJR7gHpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4209ed63-2a66-40b5-c9d7-e3f9072f9223"
      },
      "source": [
        "# vocabulary size\n",
        "len(words_df)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11147"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBNZk2uPgdiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "ae6e130d-9ee9-47c3-9595-8712389cc90e"
      },
      "source": [
        "words_df.head()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>uppermiddle</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>shoots</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>geesh</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>andrea</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>precice</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          word  count\n",
              "0  uppermiddle      1\n",
              "1       shoots      1\n",
              "2        geesh      1\n",
              "3       andrea      1\n",
              "4      precice      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFc0UDongfZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "9bf3bebc-c5ad-4373-da42-b159cc7586f6"
      },
      "source": [
        "words_df.tail()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11142</th>\n",
              "      <td>you</td>\n",
              "      <td>11909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11143</th>\n",
              "      <td>a</td>\n",
              "      <td>13380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11144</th>\n",
              "      <td>to</td>\n",
              "      <td>14000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11145</th>\n",
              "      <td>the</td>\n",
              "      <td>15406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11146</th>\n",
              "      <td>i</td>\n",
              "      <td>19654</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word  count\n",
              "11142  you  11909\n",
              "11143    a  13380\n",
              "11144   to  14000\n",
              "11145  the  15406\n",
              "11146    i  19654"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryKXcLaQgncI",
        "colab_type": "text"
      },
      "source": [
        "# **3.3 Find and Replace Rare Words with \"Unknown\" Token**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUmQZFbigqGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# user specified threshold value\n",
        "rare_thresh = 4\n",
        "\n",
        "# get percentage of rare words in the vocabulary\n",
        "rare_words_count = len(words_df[words_df['count'] < rare_thresh]['word'])\n",
        "total_words = len(words_df) \n",
        "rare_dist = rare_words_count / total_words\n",
        "\n",
        "# coverage percentage of rare words in the corpus\n",
        "rare_cover = words_df[words_df['count'] < rare_thresh]['count'].sum()/words_df['count'].sum()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjcdcfmDhGBV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "39423aca-7665-4c3b-d65f-585dc3c35434"
      },
      "source": [
        "print(f\"Rare words distribution in the vocabulary: {rare_dist*100:.2f}\")\n",
        "print(f\"Rare words coverage in the corpus: {rare_cover*100:.2f}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rare words distribution in the vocabulary: 69.03\n",
            "Rare words coverage in the corpus: 2.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrtG4ijFhUxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract rare words in a list\n",
        "rare_words = words_df[words_df['count'] < rare_thresh]['word'].tolist()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ob63VDhZcI",
        "colab_type": "text"
      },
      "source": [
        "**Let's see the technique that we will use to replace the rare words/tokens in the dataset with a special token known as the unknown token (\"\")**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88xEFuuehcV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a34e1e2-9382-4447-ff4a-c344211aa791"
      },
      "source": [
        "## example\n",
        "# specify rare words\n",
        "r_words = [\"day\", \"book\"]\n",
        "\n",
        "# build pattern\n",
        "pattern = \"\"\n",
        "for i in r_words:\n",
        "  pattern+= \"{}|\".format(i)\n",
        "\n",
        "print(pattern)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "day|book|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyaJXJ6WiFHE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2693e5d-3fb0-449e-dc53-6d3b1562f28e"
      },
      "source": [
        "# removing the last element which is \"|\"\n",
        "pattern = pattern[:-1]\n",
        "print(pattern)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "day|book\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NhLbXGsiJdb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6c413530-d6af-4a70-a632-a979fa5f8590"
      },
      "source": [
        "# replace the rare words with the <unk> token\n",
        "sents = [\"it has been a long day\", \"this book is a must read\"]\n",
        "\n",
        "for d in sents:\n",
        "  text = re.sub(pattern, \" <unk> \", d)\n",
        "  print(text)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it has been a long  <unk> \n",
            "this  <unk>  is a must read\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0aHKaRGirBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "0e807aef10ff46e69e481e407909c438",
            "309594052e8f4cc6bd56586f0ede2806",
            "1619c0818de640ec8cdf5d8f133a11d5",
            "7413574837c24c2cb47046486eaafdeb",
            "988c6e3185c540bf93aee058a7917f09",
            "ebe8732fc2a34957a983365fba265a84",
            "c1669e81f76249fab9ed0f3d542c187a",
            "46cb1d8105484c2c929ffec1b22aa45e"
          ]
        },
        "outputId": "07291fec-adc2-470a-e938-d91394c5c19e"
      },
      "source": [
        "# create a text pattern from the rare words, like \"word1 | word2 | word3...\"\n",
        "pattern = \"\"\n",
        "for i in rare_words:\n",
        "  pattern+= \" {} |\".format(i)\n",
        "\n",
        "# removing the last element which is \"|\"\n",
        "pattern = pattern[:-1]\n",
        "\n",
        "# empty list \n",
        "dialogs_clean_v2 = []\n",
        "\n",
        "# replace the rare words with the <unk> token\n",
        "for d in tqdm_notebook(dialogs_clean):\n",
        "  text = re.sub(pattern, \" <unk> \", d)\n",
        "  dialogs_clean_v2.append(text)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e807aef10ff46e69e481e407909c438",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=64776.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqmVkfYfi2Cw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "f948b0bc-4f9f-4f0c-f672-c178492bfb5e"
      },
      "source": [
        "dialogs_clean_v2[520:530]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['does it serve traditional chinese dessert',\n",
              " 'how much extra time to reach <unk> ',\n",
              " 'ok lets reserve a table for dinner at hakkasan',\n",
              " 'hello i need to get a car please',\n",
              " 'holiday inn <unk> parkconv <unk> convention center drive <unk> park il',\n",
              " 'bowling alley <unk> highway <unk> park il',\n",
              " 'what types of cars does uber have',\n",
              " \"what's the price difference\",\n",
              " 'ok get me the cheapest please',\n",
              " 'ok then get me the next level']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrif8vgLjCoe",
        "colab_type": "text"
      },
      "source": [
        "# **4. Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nGRtal2jJrC",
        "colab_type": "text"
      },
      "source": [
        "# **4.1 Prepare Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97D3JmWSjE0e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "9d376c84-f708-4445-f468-99986ffbf64b"
      },
      "source": [
        "# capture length of all the sequences\n",
        "text_word_count = []\n",
        "for i in dialogs_clean_v2:\n",
        "  text_word_count.append(len(i.split()))\n",
        "        \n",
        "# plot the sequence lengths\n",
        "pd.Series(text_word_count).hist(bins = 30,range=(0,30))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f197d0b0b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4cu8HKOjkxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to create sequences of equal length\n",
        "def create_seq(text, seq_len = 5):\n",
        "      \n",
        "  sequences = []    \n",
        "  \n",
        "  if len(text.split()) > seq_len:\n",
        "    for i in range(seq_len, len(text.split())):\n",
        "      # select sequence of tokens\n",
        "      seq = text.split()[i-seq_len:i+1]\n",
        "      # append sequence to the list\n",
        "      sequences.append(\" \".join(seq))\n",
        "\n",
        "    return sequences\n",
        "\n",
        "  else:\n",
        "    \n",
        "    return [text]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwEgrMw3jtDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create sequences of equal length\n",
        "seqs = [create_seq(i) for i in dialogs_clean_v2]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP5G6uNXjx8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "outputId": "d1bc3d89-9059-4457-e577-a11ff9aaeef0"
      },
      "source": [
        "seqs[:10]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"hi i'm looking to book a\",\n",
              "  \"i'm looking to book a table\",\n",
              "  'looking to book a table for',\n",
              "  'to book a table for korean',\n",
              "  'book a table for korean fod'],\n",
              " ['somewhere in southern nyc maybe the',\n",
              "  'in southern nyc maybe the east',\n",
              "  'southern nyc maybe the east village'],\n",
              " [\"we don't want to sit at\",\n",
              "  \"don't want to sit at the\",\n",
              "  'want to sit at the bar',\n",
              "  'to sit at the bar but',\n",
              "  'sit at the bar but anywhere',\n",
              "  'at the bar but anywhere else',\n",
              "  'the bar but anywhere else is',\n",
              "  'bar but anywhere else is fine'],\n",
              " ['what times are available'],\n",
              " [\"yikes we can't do those times\"],\n",
              " ['let me check'],\n",
              " [\"great let's book that\"],\n",
              " [\"no that's it just book\"],\n",
              " ['hi i would like to see',\n",
              "  'i would like to see if',\n",
              "  'would like to see if the',\n",
              "  'like to see if the movie',\n",
              "  'to see if the movie what',\n",
              "  'see if the movie what men',\n",
              "  'if the movie what men want',\n",
              "  'the movie what men want is',\n",
              "  'movie what men want is playing',\n",
              "  'what men want is playing here'],\n",
              " ['yes for me and a friend',\n",
              "  'for me and a friend so',\n",
              "  'me and a friend so two',\n",
              "  'and a friend so two tickets',\n",
              "  'a friend so two tickets please']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDkI5pzyj2bU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merge list-of-lists into a single list\n",
        "seqs = sum(seqs, [])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ConWN6zCkzH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "982a65a0-addb-4335-cb99-d2ff23ed2898"
      },
      "source": [
        "seqs[:15]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"hi i'm looking to book a\",\n",
              " \"i'm looking to book a table\",\n",
              " 'looking to book a table for',\n",
              " 'to book a table for korean',\n",
              " 'book a table for korean fod',\n",
              " 'somewhere in southern nyc maybe the',\n",
              " 'in southern nyc maybe the east',\n",
              " 'southern nyc maybe the east village',\n",
              " \"we don't want to sit at\",\n",
              " \"don't want to sit at the\",\n",
              " 'want to sit at the bar',\n",
              " 'to sit at the bar but',\n",
              " 'sit at the bar but anywhere',\n",
              " 'at the bar but anywhere else',\n",
              " 'the bar but anywhere else is']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-WQDop_k2qK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "659d5e88-111f-4e08-d3fa-0b77e9619153"
      },
      "source": [
        "# count of sequences\n",
        "len(seqs)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqsa4GqPlcyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create input and target sequences (x and y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for s in seqs:\n",
        "  x.append(\" \".join(s.split()[:-1]))\n",
        "  y.append(\" \".join(s.split()[1:]))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUSN1646lecW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "529690a7-c4a7-4ac3-d1b5-27e0ae58f47c"
      },
      "source": [
        "x[0], y[0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"hi i'm looking to book\", \"i'm looking to book a\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPvvcx8elk5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9f11abe-a9b7-43ea-8d84-f2abaa508351"
      },
      "source": [
        "x[88543], y[88543]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('to drive to several locations', 'drive to several locations do')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCn3SCXRltTN",
        "colab_type": "text"
      },
      "source": [
        "# **4.2 Create Token-Integer Mappings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHFQrVfalvUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create integer-to-token mapping\n",
        "int2token = {}\n",
        "cnt = 1\n",
        "\n",
        "for w in set(\" \".join(dialogs_clean_v2).split()):\n",
        "  int2token[cnt] = w\n",
        "  cnt+= 1\n",
        "\n",
        "# create token-to-integer mapping\n",
        "token2int = {t: i for i, t in int2token.items()}"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAUfyv8nl02l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "17e48c6f-38f0-4120-b30f-7c0a55293553"
      },
      "source": [
        "token2int[\"can\"], int2token[1127]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3847, 'brasa')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDS6bVuanHXv",
        "colab_type": "text"
      },
      "source": [
        "# **4.3 Split Data into Train and Validation Sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3rPZdBNnJpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train-validation split\n",
        "# input sequences\n",
        "x_tr = x[:150000]\n",
        "x_val = x[150000:]\n",
        "\n",
        "# target sequences\n",
        "y_tr = y[:150000]\n",
        "y_val = y[150000:]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP7Q5B80nVOn",
        "colab_type": "text"
      },
      "source": [
        "# **4.4 Pad Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXYlo2LznYkU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b7c46412-8dfe-4ed7-eae0-3a8d1392fa8c"
      },
      "source": [
        "# plot sequence length in train set\n",
        "text_word_count = []\n",
        "\n",
        "for i in x_tr:\n",
        "  text_word_count.append(len(i.split()))\n",
        "\n",
        "pd.Series(text_word_count).hist(bins = 70,range=(0,30))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f197f247048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh2q8TYioC8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# based on the plot above\n",
        "max_text_len = 5\n",
        "\n",
        "# function to perform padding\n",
        "def pad_sequence(seq, n):\n",
        "\n",
        "  # split input sequence into tokens\n",
        "  seq = seq.split()\n",
        "  \n",
        "  # check if no. of tokens in input sequence is less than 'n'\n",
        "  if len(seq) < n:\n",
        "    for i in range(n - len(seq)):\n",
        "      seq.append(\"<pad>\")\n",
        "\n",
        "  return \" \".join(seq)\n",
        "\n",
        "# pad text sequences (train set)\n",
        "x_tr_padded = [pad_sequence(s, max_text_len) for s in x_tr]\n",
        "y_tr_padded = [pad_sequence(s, max_text_len) for s in y_tr]\n",
        "\n",
        "# pad text sequences (validation set)\n",
        "x_val_padded = [pad_sequence(s, max_text_len) for s in x_val]\n",
        "y_val_padded = [pad_sequence(s, max_text_len) for s in y_val]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Q1SRAzoH5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "a34802fb-e55b-44ce-db39-21a2d2275af6"
      },
      "source": [
        "x_tr_padded[:20]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"hi i'm looking to book\",\n",
              " \"i'm looking to book a\",\n",
              " 'looking to book a table',\n",
              " 'to book a table for',\n",
              " 'book a table for korean',\n",
              " 'somewhere in southern nyc maybe',\n",
              " 'in southern nyc maybe the',\n",
              " 'southern nyc maybe the east',\n",
              " \"we don't want to sit\",\n",
              " \"don't want to sit at\",\n",
              " 'want to sit at the',\n",
              " 'to sit at the bar',\n",
              " 'sit at the bar but',\n",
              " 'at the bar but anywhere',\n",
              " 'the bar but anywhere else',\n",
              " 'bar but anywhere else is',\n",
              " 'what times are <pad> <pad>',\n",
              " \"yikes we can't do those\",\n",
              " 'let me <pad> <pad> <pad>',\n",
              " \"great let's book <pad> <pad>\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf4PdZz0oLms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "9b87674d-1073-4273-925a-71defee07e44"
      },
      "source": [
        "y_tr_padded[:20]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"i'm looking to book a\",\n",
              " 'looking to book a table',\n",
              " 'to book a table for',\n",
              " 'book a table for korean',\n",
              " 'a table for korean fod',\n",
              " 'in southern nyc maybe the',\n",
              " 'southern nyc maybe the east',\n",
              " 'nyc maybe the east village',\n",
              " \"don't want to sit at\",\n",
              " 'want to sit at the',\n",
              " 'to sit at the bar',\n",
              " 'sit at the bar but',\n",
              " 'at the bar but anywhere',\n",
              " 'the bar but anywhere else',\n",
              " 'bar but anywhere else is',\n",
              " 'but anywhere else is fine',\n",
              " 'times are available <pad> <pad>',\n",
              " \"we can't do those times\",\n",
              " 'me check <pad> <pad> <pad>',\n",
              " \"let's book that <pad> <pad>\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUswzRCGoPAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update mapping dictionaries\n",
        "int2token[0] = \"<pad>\"\n",
        "token2int[\"<pad>\"] = 0\n",
        "\n",
        "# set vocabulary size\n",
        "vocab_size = len(int2token)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsgXscOmoaof",
        "colab_type": "text"
      },
      "source": [
        "# **4.5 Convert Text Sequences to Integer Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJCHGwNlocxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to create integer sequences\n",
        "def get_integer_seq(seq):\n",
        "  return [token2int[w] for w in seq.split()]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG_f6GJOojiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert text sequences to integer sequences\n",
        "x_tr_int = [get_integer_seq(i) for i in x_tr_padded]\n",
        "y_tr_int = [get_integer_seq(i) for i in y_tr_padded]\n",
        "\n",
        "x_val_int = [get_integer_seq(i) for i in x_val_padded]\n",
        "y_val_int = [get_integer_seq(i) for i in y_val_padded]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp5B8Br9oqHm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "c586dc0b-e903-482a-95e0-322a4b13a657"
      },
      "source": [
        "x_tr_int[:10]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[460, 198, 5255, 2340, 4609],\n",
              " [198, 5255, 2340, 4609, 4623],\n",
              " [5255, 2340, 4609, 4623, 2996],\n",
              " [2340, 4609, 4623, 2996, 5774],\n",
              " [4609, 4623, 2996, 5774, 5589],\n",
              " [2943, 6261, 4351, 5165, 1223],\n",
              " [6261, 4351, 5165, 1223, 3109],\n",
              " [4351, 5165, 1223, 3109, 4430],\n",
              " [3787, 5177, 1544, 2340, 3153],\n",
              " [5177, 1544, 2340, 3153, 1167]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU8wLmXCorrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "ac477ecb-0d93-4d92-b47f-7a530c99899a"
      },
      "source": [
        "y_tr_int[:10]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[198, 5255, 2340, 4609, 4623],\n",
              " [5255, 2340, 4609, 4623, 2996],\n",
              " [2340, 4609, 4623, 2996, 5774],\n",
              " [4609, 4623, 2996, 5774, 5589],\n",
              " [4623, 2996, 5774, 5589, 341],\n",
              " [6261, 4351, 5165, 1223, 3109],\n",
              " [4351, 5165, 1223, 3109, 4430],\n",
              " [5165, 1223, 3109, 4430, 2556],\n",
              " [5177, 1544, 2340, 3153, 1167],\n",
              " [1544, 2340, 3153, 1167, 3109]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-w2_sieoxL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25fb7802-ed73-4237-af41-ae73e55a4908"
      },
      "source": [
        "# convert lists into numpy arrays\n",
        "x_tr_int = np.array(x_tr_int)\n",
        "y_tr_int = np.array(y_tr_int)\n",
        "\n",
        "x_val_int = np.array(x_val_int)\n",
        "y_val_int = np.array(y_val_int)\n",
        "\n",
        "x_tr_int.shape, y_tr_int.shape, x_val_int.shape, y_val_int.shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150000, 5), (150000, 5), (55346, 5), (55346, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFakxxKXo4rS",
        "colab_type": "text"
      },
      "source": [
        "# **5. Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n9goDXUo6r4",
        "colab_type": "text"
      },
      "source": [
        "# **5.1 Define Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShFFAO6To-nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model architecture\n",
        "\n",
        "## embedding layer: \n",
        "##    input dim = vocab_size, \n",
        "##    ouput dim = 200\n",
        "\n",
        "## LSTM layer:\n",
        "##    input dim = 200\n",
        "##    hidden units = 256\n",
        "##    layers = 2\n",
        "##    output dim = 256\n",
        "\n",
        "## Dropout Layer\n",
        "##    input dim = 256\n",
        "##    output dim = 256\n",
        "\n",
        "## fully connected layer\n",
        "##    input dim = 256\n",
        "##    ouput dim = vocab_size\n",
        "\n",
        "class WordLSTM(nn.Module):\n",
        "      \n",
        "  def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.3, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "    \n",
        "    self.emb_layer = nn.Embedding(vocab_size, 200)\n",
        "\n",
        "    ## define the LSTM\n",
        "    # input data is of shape (batch size, sequence length, no. of features)...\n",
        "    # ...therefore we need batch_first=True\n",
        "    self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)\n",
        "    \n",
        "    ## define a dropout layer\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    \n",
        "    ## define the fully-connected layer\n",
        "    self.fc = nn.Linear(n_hidden, vocab_size)      \n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    ''' Forward pass through the network. \n",
        "        These inputs are x, and the hidden/cell state is `hidden`. '''\n",
        "\n",
        "    ## pass input through embedding layer\n",
        "    embedded = self.emb_layer(x)  \n",
        "\n",
        "    ## Get the outputs and the new hidden state from the lstm\n",
        "    lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "    \n",
        "    ## pass through a dropout layer\n",
        "    out = self.dropout(lstm_output)\n",
        "    \n",
        "    ## reshape the tensor to the shape (batch-size*sequence length, hidden units)\n",
        "    out = out.reshape(-1, self.n_hidden)\n",
        "\n",
        "    ## put \"out\" through the fully-connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # return the final output and the hidden state\n",
        "    return out, hidden\n",
        "    \n",
        "    \n",
        "  def init_hidden(self, batch_size):\n",
        "    ''' Initializes hidden state '''\n",
        "    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "    # initialized to zero, for hidden state and cell state of LSTM\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (torch.cuda.is_available()):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE7fKZZDryuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "9b79ab97-1c9c-4b00-bfb1-c7e58a156eb8"
      },
      "source": [
        "# define and print the net\n",
        "net = WordLSTM()\n",
        "print(net)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(6502, 200)\n",
            "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=6502, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKrMhYLXr4_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to generate batches\n",
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "  # iterate through the arrays\n",
        "  prv = 0\n",
        "  \n",
        "  for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "    # batch of input sequences\n",
        "    x = arr_x[prv:n,:]\n",
        "\n",
        "    # batch of target sequences\n",
        "    y = arr_y[prv:n,:]\n",
        "\n",
        "    prv = n\n",
        "    \n",
        "    yield x, y"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOM7_yw-s19H",
        "colab_type": "text"
      },
      "source": [
        "# **5.2 Start Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN3-lL8Ts37V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, epochs=10, batch_size=32, lr=0.001, print_every=32):\n",
        "      \n",
        "  # set initial loss to infinite\n",
        "  best_valid_loss = float('inf')\n",
        "  \n",
        "  # optimizer\n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  \n",
        "  # loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "  if(torch.cuda.is_available()):\n",
        "    # push model to GPU\n",
        "    net.cuda()\n",
        "  \n",
        "  counter = 0\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for e in range(epochs):\n",
        "            \n",
        "\n",
        "    # iterate over batches\n",
        "    for x, y in get_batches(x_tr_int, y_tr_int, batch_size):\n",
        "      counter+= 1\n",
        "      \n",
        "      # convert arrays to tensors\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "      \n",
        "      if(torch.cuda.is_available()):\n",
        "        # push tensors to GPU\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      # initialize hidden state\n",
        "      h = net.init_hidden(batch_size)\n",
        "\n",
        "      # set accumulated gradients to zero\n",
        "      net.zero_grad()\n",
        "      \n",
        "      # get the output from the model\n",
        "      output, h = net(inputs, h)\n",
        "      \n",
        "      # calculate the loss and perform backprop\n",
        "      loss = criterion(output, targets.view(-1))\n",
        "      loss.backward()\n",
        "      \n",
        "      opt.step()\n",
        "      \n",
        "      if counter % print_every == 0:\n",
        "        # Get validation loss\n",
        "        \n",
        "        val_losses = []\n",
        "\n",
        "        net.eval()\n",
        "        for x, y in get_batches(x_val_int, y_val_int, batch_size):\n",
        "            \n",
        "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "          \n",
        "          val_h = net.init_hidden(batch_size)\n",
        "\n",
        "          inputs, targets = x, y\n",
        "          if(torch.cuda.is_available()):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "          output, val_h = net(inputs, val_h)\n",
        "\n",
        "          val_loss = criterion(output, targets.view(-1))\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        #save the best model\n",
        "        if np.mean(val_losses) < best_valid_loss:\n",
        "          best_valid_loss = np.mean(val_losses)\n",
        "          torch.save(net.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "        net.train()\n",
        "\n",
        "      \n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "              \"Step: {}...\".format(counter),\n",
        "              \"Loss: {:.4f}...\".format(loss.item()),\n",
        "              \"ppl: {:.4f} \".format(np.exp(np.mean(val_losses))),\n",
        "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEhMmeKrtDBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f8bb396-089d-45a8-886e-8ca7eb3fd072"
      },
      "source": [
        "# specify batch size\n",
        "batch_size = 64\n",
        "\n",
        "# train the model\n",
        "train(net, batch_size = batch_size, epochs=10)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 32... Loss: 6.4784... ppl: 670.9487  Val Loss: 6.5087\n",
            "Epoch: 1/10... Step: 64... Loss: 5.6409... ppl: 343.3518  Val Loss: 5.8388\n",
            "Epoch: 1/10... Step: 96... Loss: 6.0772... ppl: 276.1961  Val Loss: 5.6211\n",
            "Epoch: 1/10... Step: 128... Loss: 5.2709... ppl: 239.6580  Val Loss: 5.4792\n",
            "Epoch: 1/10... Step: 160... Loss: 6.2045... ppl: 211.6844  Val Loss: 5.3551\n",
            "Epoch: 1/10... Step: 192... Loss: 5.7582... ppl: 189.2365  Val Loss: 5.2430\n",
            "Epoch: 1/10... Step: 224... Loss: 5.1731... ppl: 172.1208  Val Loss: 5.1482\n",
            "Epoch: 1/10... Step: 256... Loss: 4.3078... ppl: 157.8676  Val Loss: 5.0618\n",
            "Epoch: 1/10... Step: 288... Loss: 4.8329... ppl: 145.5787  Val Loss: 4.9807\n",
            "Epoch: 1/10... Step: 320... Loss: 4.9814... ppl: 136.7871  Val Loss: 4.9184\n",
            "Epoch: 1/10... Step: 352... Loss: 4.4209... ppl: 127.5081  Val Loss: 4.8482\n",
            "Epoch: 1/10... Step: 384... Loss: 5.1342... ppl: 118.4260  Val Loss: 4.7743\n",
            "Epoch: 1/10... Step: 416... Loss: 4.9139... ppl: 112.6923  Val Loss: 4.7247\n",
            "Epoch: 1/10... Step: 448... Loss: 4.5010... ppl: 107.9693  Val Loss: 4.6818\n",
            "Epoch: 1/10... Step: 480... Loss: 4.2893... ppl: 103.2134  Val Loss: 4.6368\n",
            "Epoch: 1/10... Step: 512... Loss: 4.5103... ppl: 99.1157  Val Loss: 4.5963\n",
            "Epoch: 1/10... Step: 544... Loss: 4.7495... ppl: 94.9822  Val Loss: 4.5537\n",
            "Epoch: 1/10... Step: 576... Loss: 5.5720... ppl: 92.1277  Val Loss: 4.5232\n",
            "Epoch: 1/10... Step: 608... Loss: 4.4664... ppl: 89.4279  Val Loss: 4.4934\n",
            "Epoch: 1/10... Step: 640... Loss: 4.4525... ppl: 86.0446  Val Loss: 4.4549\n",
            "Epoch: 1/10... Step: 672... Loss: 4.4401... ppl: 84.1955  Val Loss: 4.4331\n",
            "Epoch: 1/10... Step: 704... Loss: 4.2903... ppl: 81.6750  Val Loss: 4.4027\n",
            "Epoch: 1/10... Step: 736... Loss: 4.8358... ppl: 79.7417  Val Loss: 4.3788\n",
            "Epoch: 1/10... Step: 768... Loss: 3.8871... ppl: 77.1897  Val Loss: 4.3463\n",
            "Epoch: 1/10... Step: 800... Loss: 4.1634... ppl: 75.6831  Val Loss: 4.3266\n",
            "Epoch: 1/10... Step: 832... Loss: 4.7390... ppl: 74.2270  Val Loss: 4.3071\n",
            "Epoch: 1/10... Step: 864... Loss: 4.1827... ppl: 72.5911  Val Loss: 4.2848\n",
            "Epoch: 1/10... Step: 896... Loss: 4.3700... ppl: 71.0497  Val Loss: 4.2634\n",
            "Epoch: 1/10... Step: 928... Loss: 4.3174... ppl: 69.0720  Val Loss: 4.2351\n",
            "Epoch: 1/10... Step: 960... Loss: 3.9701... ppl: 68.6610  Val Loss: 4.2292\n",
            "Epoch: 1/10... Step: 992... Loss: 3.7681... ppl: 66.6699  Val Loss: 4.1998\n",
            "Epoch: 1/10... Step: 1024... Loss: 3.7271... ppl: 65.5854  Val Loss: 4.1834\n",
            "Epoch: 1/10... Step: 1056... Loss: 4.2730... ppl: 64.3652  Val Loss: 4.1646\n",
            "Epoch: 1/10... Step: 1088... Loss: 3.4171... ppl: 63.2668  Val Loss: 4.1474\n",
            "Epoch: 1/10... Step: 1120... Loss: 4.1234... ppl: 62.5383  Val Loss: 4.1358\n",
            "Epoch: 1/10... Step: 1152... Loss: 3.2145... ppl: 61.8392  Val Loss: 4.1245\n",
            "Epoch: 1/10... Step: 1184... Loss: 3.9986... ppl: 60.7169  Val Loss: 4.1062\n",
            "Epoch: 1/10... Step: 1216... Loss: 3.8144... ppl: 60.0526  Val Loss: 4.0952\n",
            "Epoch: 1/10... Step: 1248... Loss: 4.1834... ppl: 58.9478  Val Loss: 4.0767\n",
            "Epoch: 1/10... Step: 1280... Loss: 4.2392... ppl: 58.0436  Val Loss: 4.0612\n",
            "Epoch: 1/10... Step: 1312... Loss: 4.7579... ppl: 58.0795  Val Loss: 4.0618\n",
            "Epoch: 1/10... Step: 1344... Loss: 3.8750... ppl: 57.0597  Val Loss: 4.0441\n",
            "Epoch: 1/10... Step: 1376... Loss: 4.1867... ppl: 56.1761  Val Loss: 4.0285\n",
            "Epoch: 1/10... Step: 1408... Loss: 3.6084... ppl: 55.3961  Val Loss: 4.0145\n",
            "Epoch: 1/10... Step: 1440... Loss: 4.1338... ppl: 54.9559  Val Loss: 4.0065\n",
            "Epoch: 1/10... Step: 1472... Loss: 3.4082... ppl: 54.4301  Val Loss: 3.9969\n",
            "Epoch: 1/10... Step: 1504... Loss: 4.0489... ppl: 54.1529  Val Loss: 3.9918\n",
            "Epoch: 1/10... Step: 1536... Loss: 4.8179... ppl: 53.7874  Val Loss: 3.9850\n",
            "Epoch: 1/10... Step: 1568... Loss: 3.9794... ppl: 52.5576  Val Loss: 3.9619\n",
            "Epoch: 1/10... Step: 1600... Loss: 3.7087... ppl: 52.0915  Val Loss: 3.9530\n",
            "Epoch: 1/10... Step: 1632... Loss: 5.1523... ppl: 51.4977  Val Loss: 3.9415\n",
            "Epoch: 1/10... Step: 1664... Loss: 4.0289... ppl: 50.9997  Val Loss: 3.9318\n",
            "Epoch: 1/10... Step: 1696... Loss: 3.6458... ppl: 50.7832  Val Loss: 3.9276\n",
            "Epoch: 1/10... Step: 1728... Loss: 3.7042... ppl: 50.1959  Val Loss: 3.9159\n",
            "Epoch: 1/10... Step: 1760... Loss: 4.5128... ppl: 50.1563  Val Loss: 3.9151\n",
            "Epoch: 1/10... Step: 1792... Loss: 3.4650... ppl: 49.6767  Val Loss: 3.9055\n",
            "Epoch: 1/10... Step: 1824... Loss: 4.4369... ppl: 49.9264  Val Loss: 3.9106\n",
            "Epoch: 1/10... Step: 1856... Loss: 5.7769... ppl: 48.6730  Val Loss: 3.8851\n",
            "Epoch: 1/10... Step: 1888... Loss: 3.6340... ppl: 48.0863  Val Loss: 3.8730\n",
            "Epoch: 1/10... Step: 1920... Loss: 3.5396... ppl: 47.7255  Val Loss: 3.8655\n",
            "Epoch: 1/10... Step: 1952... Loss: 2.9578... ppl: 47.4103  Val Loss: 3.8588\n",
            "Epoch: 1/10... Step: 1984... Loss: 2.9035... ppl: 47.3502  Val Loss: 3.8576\n",
            "Epoch: 1/10... Step: 2016... Loss: 3.4933... ppl: 46.6210  Val Loss: 3.8421\n",
            "Epoch: 1/10... Step: 2048... Loss: 4.0786... ppl: 46.3788  Val Loss: 3.8368\n",
            "Epoch: 1/10... Step: 2080... Loss: 3.8078... ppl: 46.0846  Val Loss: 3.8305\n",
            "Epoch: 1/10... Step: 2112... Loss: 4.7814... ppl: 46.2160  Val Loss: 3.8333\n",
            "Epoch: 1/10... Step: 2144... Loss: 3.4523... ppl: 45.7312  Val Loss: 3.8228\n",
            "Epoch: 1/10... Step: 2176... Loss: 5.0769... ppl: 45.4533  Val Loss: 3.8167\n",
            "Epoch: 1/10... Step: 2208... Loss: 4.1842... ppl: 45.4283  Val Loss: 3.8161\n",
            "Epoch: 1/10... Step: 2240... Loss: 3.6289... ppl: 45.2185  Val Loss: 3.8115\n",
            "Epoch: 1/10... Step: 2272... Loss: 3.9255... ppl: 44.5162  Val Loss: 3.7959\n",
            "Epoch: 1/10... Step: 2304... Loss: 3.2838... ppl: 44.2404  Val Loss: 3.7896\n",
            "Epoch: 1/10... Step: 2336... Loss: 4.2684... ppl: 44.1210  Val Loss: 3.7869\n",
            "Epoch: 2/10... Step: 2368... Loss: 3.2359... ppl: 44.2635  Val Loss: 3.7902\n",
            "Epoch: 2/10... Step: 2400... Loss: 3.4329... ppl: 43.8699  Val Loss: 3.7812\n",
            "Epoch: 2/10... Step: 2432... Loss: 3.8245... ppl: 44.0051  Val Loss: 3.7843\n",
            "Epoch: 2/10... Step: 2464... Loss: 4.2428... ppl: 43.7346  Val Loss: 3.7781\n",
            "Epoch: 2/10... Step: 2496... Loss: 4.0138... ppl: 43.2938  Val Loss: 3.7680\n",
            "Epoch: 2/10... Step: 2528... Loss: 3.5484... ppl: 43.1801  Val Loss: 3.7654\n",
            "Epoch: 2/10... Step: 2560... Loss: 5.3485... ppl: 43.0020  Val Loss: 3.7612\n",
            "Epoch: 2/10... Step: 2592... Loss: 4.1048... ppl: 43.2385  Val Loss: 3.7667\n",
            "Epoch: 2/10... Step: 2624... Loss: 3.1920... ppl: 42.7706  Val Loss: 3.7559\n",
            "Epoch: 2/10... Step: 2656... Loss: 3.8227... ppl: 42.5216  Val Loss: 3.7500\n",
            "Epoch: 2/10... Step: 2688... Loss: 3.3183... ppl: 42.2109  Val Loss: 3.7427\n",
            "Epoch: 2/10... Step: 2720... Loss: 3.7397... ppl: 41.9186  Val Loss: 3.7357\n",
            "Epoch: 2/10... Step: 2752... Loss: 3.5896... ppl: 42.0694  Val Loss: 3.7393\n",
            "Epoch: 2/10... Step: 2784... Loss: 4.2725... ppl: 41.9589  Val Loss: 3.7367\n",
            "Epoch: 2/10... Step: 2816... Loss: 4.1166... ppl: 41.9101  Val Loss: 3.7355\n",
            "Epoch: 2/10... Step: 2848... Loss: 3.6164... ppl: 41.3790  Val Loss: 3.7228\n",
            "Epoch: 2/10... Step: 2880... Loss: 4.1494... ppl: 41.4837  Val Loss: 3.7253\n",
            "Epoch: 2/10... Step: 2912... Loss: 3.2684... ppl: 41.0806  Val Loss: 3.7155\n",
            "Epoch: 2/10... Step: 2944... Loss: 3.3511... ppl: 41.1825  Val Loss: 3.7180\n",
            "Epoch: 2/10... Step: 2976... Loss: 3.4097... ppl: 40.9075  Val Loss: 3.7113\n",
            "Epoch: 2/10... Step: 3008... Loss: 4.1788... ppl: 40.8403  Val Loss: 3.7097\n",
            "Epoch: 2/10... Step: 3040... Loss: 3.4828... ppl: 40.6209  Val Loss: 3.7043\n",
            "Epoch: 2/10... Step: 3072... Loss: 3.6621... ppl: 40.5597  Val Loss: 3.7028\n",
            "Epoch: 2/10... Step: 3104... Loss: 3.7549... ppl: 40.3926  Val Loss: 3.6986\n",
            "Epoch: 2/10... Step: 3136... Loss: 4.0948... ppl: 40.1532  Val Loss: 3.6927\n",
            "Epoch: 2/10... Step: 3168... Loss: 3.4336... ppl: 40.0188  Val Loss: 3.6893\n",
            "Epoch: 2/10... Step: 3200... Loss: 3.7252... ppl: 40.1980  Val Loss: 3.6938\n",
            "Epoch: 2/10... Step: 3232... Loss: 4.1876... ppl: 39.9296  Val Loss: 3.6871\n",
            "Epoch: 2/10... Step: 3264... Loss: 2.8649... ppl: 39.6702  Val Loss: 3.6806\n",
            "Epoch: 2/10... Step: 3296... Loss: 4.2274... ppl: 39.6535  Val Loss: 3.6802\n",
            "Epoch: 2/10... Step: 3328... Loss: 4.0759... ppl: 39.5078  Val Loss: 3.6765\n",
            "Epoch: 2/10... Step: 3360... Loss: 3.6225... ppl: 39.1730  Val Loss: 3.6680\n",
            "Epoch: 2/10... Step: 3392... Loss: 3.2583... ppl: 39.4012  Val Loss: 3.6738\n",
            "Epoch: 2/10... Step: 3424... Loss: 4.3003... ppl: 39.1318  Val Loss: 3.6669\n",
            "Epoch: 2/10... Step: 3456... Loss: 3.5282... ppl: 38.9694  Val Loss: 3.6628\n",
            "Epoch: 2/10... Step: 3488... Loss: 3.5574... ppl: 39.1077  Val Loss: 3.6663\n",
            "Epoch: 2/10... Step: 3520... Loss: 3.4536... ppl: 38.6468  Val Loss: 3.6545\n",
            "Epoch: 2/10... Step: 3552... Loss: 4.1571... ppl: 38.7839  Val Loss: 3.6580\n",
            "Epoch: 2/10... Step: 3584... Loss: 3.0014... ppl: 38.5636  Val Loss: 3.6523\n",
            "Epoch: 2/10... Step: 3616... Loss: 3.6668... ppl: 38.4585  Val Loss: 3.6496\n",
            "Epoch: 2/10... Step: 3648... Loss: 3.2453... ppl: 38.4961  Val Loss: 3.6506\n",
            "Epoch: 2/10... Step: 3680... Loss: 3.6979... ppl: 38.4929  Val Loss: 3.6505\n",
            "Epoch: 2/10... Step: 3712... Loss: 3.4626... ppl: 38.2803  Val Loss: 3.6449\n",
            "Epoch: 2/10... Step: 3744... Loss: 3.4643... ppl: 38.0668  Val Loss: 3.6393\n",
            "Epoch: 2/10... Step: 3776... Loss: 3.9174... ppl: 38.1861  Val Loss: 3.6425\n",
            "Epoch: 2/10... Step: 3808... Loss: 4.9879... ppl: 37.8869  Val Loss: 3.6346\n",
            "Epoch: 2/10... Step: 3840... Loss: 4.2452... ppl: 38.0583  Val Loss: 3.6391\n",
            "Epoch: 2/10... Step: 3872... Loss: 3.8892... ppl: 37.8092  Val Loss: 3.6326\n",
            "Epoch: 2/10... Step: 3904... Loss: 2.9327... ppl: 37.6943  Val Loss: 3.6295\n",
            "Epoch: 2/10... Step: 3936... Loss: 3.7948... ppl: 37.8191  Val Loss: 3.6328\n",
            "Epoch: 2/10... Step: 3968... Loss: 3.9851... ppl: 37.3831  Val Loss: 3.6212\n",
            "Epoch: 2/10... Step: 4000... Loss: 3.6710... ppl: 37.3054  Val Loss: 3.6191\n",
            "Epoch: 2/10... Step: 4032... Loss: 4.1724... ppl: 37.3068  Val Loss: 3.6192\n",
            "Epoch: 2/10... Step: 4064... Loss: 3.7194... ppl: 37.2704  Val Loss: 3.6182\n",
            "Epoch: 2/10... Step: 4096... Loss: 3.5298... ppl: 37.2216  Val Loss: 3.6169\n",
            "Epoch: 2/10... Step: 4128... Loss: 2.8975... ppl: 37.2086  Val Loss: 3.6165\n",
            "Epoch: 2/10... Step: 4160... Loss: 2.9670... ppl: 37.3334  Val Loss: 3.6199\n",
            "Epoch: 2/10... Step: 4192... Loss: 3.4474... ppl: 36.9761  Val Loss: 3.6103\n",
            "Epoch: 2/10... Step: 4224... Loss: 3.3430... ppl: 36.7092  Val Loss: 3.6030\n",
            "Epoch: 2/10... Step: 4256... Loss: 3.4446... ppl: 36.7902  Val Loss: 3.6052\n",
            "Epoch: 2/10... Step: 4288... Loss: 3.4262... ppl: 36.5014  Val Loss: 3.5974\n",
            "Epoch: 2/10... Step: 4320... Loss: 2.8584... ppl: 36.5824  Val Loss: 3.5996\n",
            "Epoch: 2/10... Step: 4352... Loss: 2.9802... ppl: 36.5424  Val Loss: 3.5985\n",
            "Epoch: 2/10... Step: 4384... Loss: 3.1630... ppl: 36.4244  Val Loss: 3.5952\n",
            "Epoch: 2/10... Step: 4416... Loss: 3.8162... ppl: 36.3540  Val Loss: 3.5933\n",
            "Epoch: 2/10... Step: 4448... Loss: 2.8739... ppl: 36.5344  Val Loss: 3.5983\n",
            "Epoch: 2/10... Step: 4480... Loss: 4.3930... ppl: 36.3850  Val Loss: 3.5942\n",
            "Epoch: 2/10... Step: 4512... Loss: 3.3179... ppl: 36.2377  Val Loss: 3.5901\n",
            "Epoch: 2/10... Step: 4544... Loss: 3.5057... ppl: 36.2056  Val Loss: 3.5892\n",
            "Epoch: 2/10... Step: 4576... Loss: 2.8022... ppl: 36.3143  Val Loss: 3.5922\n",
            "Epoch: 2/10... Step: 4608... Loss: 2.9277... ppl: 36.1404  Val Loss: 3.5874\n",
            "Epoch: 2/10... Step: 4640... Loss: 3.7347... ppl: 35.9995  Val Loss: 3.5835\n",
            "Epoch: 2/10... Step: 4672... Loss: 3.5763... ppl: 35.8842  Val Loss: 3.5803\n",
            "Epoch: 3/10... Step: 4704... Loss: 3.0639... ppl: 35.8366  Val Loss: 3.5790\n",
            "Epoch: 3/10... Step: 4736... Loss: 4.2060... ppl: 36.0683  Val Loss: 3.5854\n",
            "Epoch: 3/10... Step: 4768... Loss: 3.1241... ppl: 35.9578  Val Loss: 3.5823\n",
            "Epoch: 3/10... Step: 4800... Loss: 2.6248... ppl: 36.0436  Val Loss: 3.5847\n",
            "Epoch: 3/10... Step: 4832... Loss: 3.3087... ppl: 35.8011  Val Loss: 3.5780\n",
            "Epoch: 3/10... Step: 4864... Loss: 3.6904... ppl: 35.8344  Val Loss: 3.5789\n",
            "Epoch: 3/10... Step: 4896... Loss: 2.4913... ppl: 35.8731  Val Loss: 3.5800\n",
            "Epoch: 3/10... Step: 4928... Loss: 2.0968... ppl: 35.9350  Val Loss: 3.5817\n",
            "Epoch: 3/10... Step: 4960... Loss: 3.2684... ppl: 35.9332  Val Loss: 3.5817\n",
            "Epoch: 3/10... Step: 4992... Loss: 3.4578... ppl: 36.1070  Val Loss: 3.5865\n",
            "Epoch: 3/10... Step: 5024... Loss: 3.3502... ppl: 35.7313  Val Loss: 3.5760\n",
            "Epoch: 3/10... Step: 5056... Loss: 3.0636... ppl: 35.5579  Val Loss: 3.5712\n",
            "Epoch: 3/10... Step: 5088... Loss: 2.8352... ppl: 35.5375  Val Loss: 3.5706\n",
            "Epoch: 3/10... Step: 5120... Loss: 3.8027... ppl: 35.7220  Val Loss: 3.5758\n",
            "Epoch: 3/10... Step: 5152... Loss: 2.8206... ppl: 35.8227  Val Loss: 3.5786\n",
            "Epoch: 3/10... Step: 5184... Loss: 2.8858... ppl: 35.6294  Val Loss: 3.5732\n",
            "Epoch: 3/10... Step: 5216... Loss: 2.9506... ppl: 35.3273  Val Loss: 3.5647\n",
            "Epoch: 3/10... Step: 5248... Loss: 3.0083... ppl: 35.3687  Val Loss: 3.5658\n",
            "Epoch: 3/10... Step: 5280... Loss: 3.2914... ppl: 35.5435  Val Loss: 3.5708\n",
            "Epoch: 3/10... Step: 5312... Loss: 3.5285... ppl: 35.4495  Val Loss: 3.5681\n",
            "Epoch: 3/10... Step: 5344... Loss: 3.6548... ppl: 35.3385  Val Loss: 3.5650\n",
            "Epoch: 3/10... Step: 5376... Loss: 4.1088... ppl: 35.3716  Val Loss: 3.5659\n",
            "Epoch: 3/10... Step: 5408... Loss: 3.8447... ppl: 35.3340  Val Loss: 3.5648\n",
            "Epoch: 3/10... Step: 5440... Loss: 4.4475... ppl: 35.3169  Val Loss: 3.5644\n",
            "Epoch: 3/10... Step: 5472... Loss: 3.8225... ppl: 35.0737  Val Loss: 3.5575\n",
            "Epoch: 3/10... Step: 5504... Loss: 3.5538... ppl: 35.1070  Val Loss: 3.5584\n",
            "Epoch: 3/10... Step: 5536... Loss: 3.2340... ppl: 35.3428  Val Loss: 3.5651\n",
            "Epoch: 3/10... Step: 5568... Loss: 3.0066... ppl: 34.9951  Val Loss: 3.5552\n",
            "Epoch: 3/10... Step: 5600... Loss: 3.4018... ppl: 35.0853  Val Loss: 3.5578\n",
            "Epoch: 3/10... Step: 5632... Loss: 3.3711... ppl: 35.0477  Val Loss: 3.5567\n",
            "Epoch: 3/10... Step: 5664... Loss: 3.1967... ppl: 35.0643  Val Loss: 3.5572\n",
            "Epoch: 3/10... Step: 5696... Loss: 3.7070... ppl: 34.9157  Val Loss: 3.5529\n",
            "Epoch: 3/10... Step: 5728... Loss: 3.2702... ppl: 35.2352  Val Loss: 3.5620\n",
            "Epoch: 3/10... Step: 5760... Loss: 3.7567... ppl: 34.9501  Val Loss: 3.5539\n",
            "Epoch: 3/10... Step: 5792... Loss: 3.9112... ppl: 34.8017  Val Loss: 3.5497\n",
            "Epoch: 3/10... Step: 5824... Loss: 3.2753... ppl: 34.8271  Val Loss: 3.5504\n",
            "Epoch: 3/10... Step: 5856... Loss: 3.3535... ppl: 34.5838  Val Loss: 3.5434\n",
            "Epoch: 3/10... Step: 5888... Loss: 3.1144... ppl: 34.7397  Val Loss: 3.5479\n",
            "Epoch: 3/10... Step: 5920... Loss: 3.5072... ppl: 34.5477  Val Loss: 3.5423\n",
            "Epoch: 3/10... Step: 5952... Loss: 3.9231... ppl: 34.4999  Val Loss: 3.5410\n",
            "Epoch: 3/10... Step: 5984... Loss: 3.2160... ppl: 34.5217  Val Loss: 3.5416\n",
            "Epoch: 3/10... Step: 6016... Loss: 3.7859... ppl: 34.5212  Val Loss: 3.5416\n",
            "Epoch: 3/10... Step: 6048... Loss: 3.4038... ppl: 34.8547  Val Loss: 3.5512\n",
            "Epoch: 3/10... Step: 6080... Loss: 2.8048... ppl: 34.4648  Val Loss: 3.5399\n",
            "Epoch: 3/10... Step: 6112... Loss: 3.3037... ppl: 34.4608  Val Loss: 3.5398\n",
            "Epoch: 3/10... Step: 6144... Loss: 3.1396... ppl: 34.2867  Val Loss: 3.5348\n",
            "Epoch: 3/10... Step: 6176... Loss: 3.1764... ppl: 34.3674  Val Loss: 3.5371\n",
            "Epoch: 3/10... Step: 6208... Loss: 3.4159... ppl: 34.3412  Val Loss: 3.5363\n",
            "Epoch: 3/10... Step: 6240... Loss: 2.7504... ppl: 34.3107  Val Loss: 3.5355\n",
            "Epoch: 3/10... Step: 6272... Loss: 2.7017... ppl: 34.5114  Val Loss: 3.5413\n",
            "Epoch: 3/10... Step: 6304... Loss: 3.1410... ppl: 34.1858  Val Loss: 3.5318\n",
            "Epoch: 3/10... Step: 6336... Loss: 3.5179... ppl: 34.0489  Val Loss: 3.5278\n",
            "Epoch: 3/10... Step: 6368... Loss: 3.1537... ppl: 34.1834  Val Loss: 3.5317\n",
            "Epoch: 3/10... Step: 6400... Loss: 3.7590... ppl: 34.1863  Val Loss: 3.5318\n",
            "Epoch: 3/10... Step: 6432... Loss: 3.5686... ppl: 34.1148  Val Loss: 3.5297\n",
            "Epoch: 3/10... Step: 6464... Loss: 3.6088... ppl: 34.2717  Val Loss: 3.5343\n",
            "Epoch: 3/10... Step: 6496... Loss: 2.8471... ppl: 34.1443  Val Loss: 3.5306\n",
            "Epoch: 3/10... Step: 6528... Loss: 3.0770... ppl: 34.1010  Val Loss: 3.5293\n",
            "Epoch: 3/10... Step: 6560... Loss: 3.2951... ppl: 33.9918  Val Loss: 3.5261\n",
            "Epoch: 3/10... Step: 6592... Loss: 2.5139... ppl: 34.1094  Val Loss: 3.5296\n",
            "Epoch: 3/10... Step: 6624... Loss: 3.6170... ppl: 33.7942  Val Loss: 3.5203\n",
            "Epoch: 3/10... Step: 6656... Loss: 3.0093... ppl: 33.8242  Val Loss: 3.5212\n",
            "Epoch: 3/10... Step: 6688... Loss: 2.8299... ppl: 34.0227  Val Loss: 3.5270\n",
            "Epoch: 3/10... Step: 6720... Loss: 2.4186... ppl: 33.8399  Val Loss: 3.5216\n",
            "Epoch: 3/10... Step: 6752... Loss: 3.4837... ppl: 33.7925  Val Loss: 3.5202\n",
            "Epoch: 3/10... Step: 6784... Loss: 3.2313... ppl: 33.9179  Val Loss: 3.5239\n",
            "Epoch: 3/10... Step: 6816... Loss: 2.7735... ppl: 33.9038  Val Loss: 3.5235\n",
            "Epoch: 3/10... Step: 6848... Loss: 2.8141... ppl: 33.7130  Val Loss: 3.5179\n",
            "Epoch: 3/10... Step: 6880... Loss: 3.0178... ppl: 33.8283  Val Loss: 3.5213\n",
            "Epoch: 3/10... Step: 6912... Loss: 4.0859... ppl: 33.9500  Val Loss: 3.5249\n",
            "Epoch: 3/10... Step: 6944... Loss: 3.2077... ppl: 34.0899  Val Loss: 3.5290\n",
            "Epoch: 3/10... Step: 6976... Loss: 3.4098... ppl: 33.7678  Val Loss: 3.5195\n",
            "Epoch: 3/10... Step: 7008... Loss: 3.4178... ppl: 33.6494  Val Loss: 3.5160\n",
            "Epoch: 4/10... Step: 7040... Loss: 3.4794... ppl: 33.5278  Val Loss: 3.5124\n",
            "Epoch: 4/10... Step: 7072... Loss: 2.9073... ppl: 33.9362  Val Loss: 3.5245\n",
            "Epoch: 4/10... Step: 7104... Loss: 3.2222... ppl: 33.6002  Val Loss: 3.5145\n",
            "Epoch: 4/10... Step: 7136... Loss: 2.8477... ppl: 33.7464  Val Loss: 3.5189\n",
            "Epoch: 4/10... Step: 7168... Loss: 3.4268... ppl: 33.6663  Val Loss: 3.5165\n",
            "Epoch: 4/10... Step: 7200... Loss: 2.6418... ppl: 33.7351  Val Loss: 3.5185\n",
            "Epoch: 4/10... Step: 7232... Loss: 3.0212... ppl: 33.8464  Val Loss: 3.5218\n",
            "Epoch: 4/10... Step: 7264... Loss: 4.1811... ppl: 33.8507  Val Loss: 3.5220\n",
            "Epoch: 4/10... Step: 7296... Loss: 3.5111... ppl: 34.0300  Val Loss: 3.5272\n",
            "Epoch: 4/10... Step: 7328... Loss: 2.5942... ppl: 34.1764  Val Loss: 3.5315\n",
            "Epoch: 4/10... Step: 7360... Loss: 2.8941... ppl: 33.9994  Val Loss: 3.5263\n",
            "Epoch: 4/10... Step: 7392... Loss: 3.2062... ppl: 33.6957  Val Loss: 3.5174\n",
            "Epoch: 4/10... Step: 7424... Loss: 2.6725... ppl: 33.6296  Val Loss: 3.5154\n",
            "Epoch: 4/10... Step: 7456... Loss: 2.7567... ppl: 33.8747  Val Loss: 3.5227\n",
            "Epoch: 4/10... Step: 7488... Loss: 3.5441... ppl: 34.0180  Val Loss: 3.5269\n",
            "Epoch: 4/10... Step: 7520... Loss: 2.1065... ppl: 34.1153  Val Loss: 3.5297\n",
            "Epoch: 4/10... Step: 7552... Loss: 3.1376... ppl: 33.7909  Val Loss: 3.5202\n",
            "Epoch: 4/10... Step: 7584... Loss: 2.6364... ppl: 33.9282  Val Loss: 3.5242\n",
            "Epoch: 4/10... Step: 7616... Loss: 2.9901... ppl: 33.8964  Val Loss: 3.5233\n",
            "Epoch: 4/10... Step: 7648... Loss: 3.4630... ppl: 33.9093  Val Loss: 3.5237\n",
            "Epoch: 4/10... Step: 7680... Loss: 2.9019... ppl: 33.9037  Val Loss: 3.5235\n",
            "Epoch: 4/10... Step: 7712... Loss: 2.6932... ppl: 33.8189  Val Loss: 3.5210\n",
            "Epoch: 4/10... Step: 7744... Loss: 3.3708... ppl: 33.7769  Val Loss: 3.5198\n",
            "Epoch: 4/10... Step: 7776... Loss: 3.6607... ppl: 33.8374  Val Loss: 3.5216\n",
            "Epoch: 4/10... Step: 7808... Loss: 3.4837... ppl: 33.5986  Val Loss: 3.5145\n",
            "Epoch: 4/10... Step: 7840... Loss: 2.2030... ppl: 33.7556  Val Loss: 3.5191\n",
            "Epoch: 4/10... Step: 7872... Loss: 3.7135... ppl: 33.8032  Val Loss: 3.5206\n",
            "Epoch: 4/10... Step: 7904... Loss: 2.3375... ppl: 33.6509  Val Loss: 3.5160\n",
            "Epoch: 4/10... Step: 7936... Loss: 3.1222... ppl: 33.6997  Val Loss: 3.5175\n",
            "Epoch: 4/10... Step: 7968... Loss: 3.8450... ppl: 33.6783  Val Loss: 3.5169\n",
            "Epoch: 4/10... Step: 8000... Loss: 2.9498... ppl: 33.7610  Val Loss: 3.5193\n",
            "Epoch: 4/10... Step: 8032... Loss: 3.3712... ppl: 33.7014  Val Loss: 3.5175\n",
            "Epoch: 4/10... Step: 8064... Loss: 2.4647... ppl: 33.8387  Val Loss: 3.5216\n",
            "Epoch: 4/10... Step: 8096... Loss: 3.0843... ppl: 33.7299  Val Loss: 3.5184\n",
            "Epoch: 4/10... Step: 8128... Loss: 2.2805... ppl: 33.5242  Val Loss: 3.5123\n",
            "Epoch: 4/10... Step: 8160... Loss: 2.9733... ppl: 33.5721  Val Loss: 3.5137\n",
            "Epoch: 4/10... Step: 8192... Loss: 3.3906... ppl: 33.4926  Val Loss: 3.5113\n",
            "Epoch: 4/10... Step: 8224... Loss: 2.8994... ppl: 33.5196  Val Loss: 3.5121\n",
            "Epoch: 4/10... Step: 8256... Loss: 2.6959... ppl: 33.4500  Val Loss: 3.5101\n",
            "Epoch: 4/10... Step: 8288... Loss: 3.1119... ppl: 33.5397  Val Loss: 3.5127\n",
            "Epoch: 4/10... Step: 8320... Loss: 3.1823... ppl: 33.3442  Val Loss: 3.5069\n",
            "Epoch: 4/10... Step: 8352... Loss: 2.8370... ppl: 33.3664  Val Loss: 3.5076\n",
            "Epoch: 4/10... Step: 8384... Loss: 2.5820... ppl: 33.7586  Val Loss: 3.5192\n",
            "Epoch: 4/10... Step: 8416... Loss: 2.7727... ppl: 33.4843  Val Loss: 3.5111\n",
            "Epoch: 4/10... Step: 8448... Loss: 4.0673... ppl: 33.4111  Val Loss: 3.5089\n",
            "Epoch: 4/10... Step: 8480... Loss: 2.8091... ppl: 33.3862  Val Loss: 3.5081\n",
            "Epoch: 4/10... Step: 8512... Loss: 2.4730... ppl: 33.3634  Val Loss: 3.5075\n",
            "Epoch: 4/10... Step: 8544... Loss: 3.0397... ppl: 33.4330  Val Loss: 3.5095\n",
            "Epoch: 4/10... Step: 8576... Loss: 2.9651... ppl: 33.3995  Val Loss: 3.5085\n",
            "Epoch: 4/10... Step: 8608... Loss: 3.4747... ppl: 33.4740  Val Loss: 3.5108\n",
            "Epoch: 4/10... Step: 8640... Loss: 3.2440... ppl: 33.3204  Val Loss: 3.5062\n",
            "Epoch: 4/10... Step: 8672... Loss: 2.7125... ppl: 33.2236  Val Loss: 3.5033\n",
            "Epoch: 4/10... Step: 8704... Loss: 3.6843... ppl: 33.1343  Val Loss: 3.5006\n",
            "Epoch: 4/10... Step: 8736... Loss: 2.9605... ppl: 33.2263  Val Loss: 3.5033\n",
            "Epoch: 4/10... Step: 8768... Loss: 2.7128... ppl: 33.2160  Val Loss: 3.5030\n",
            "Epoch: 4/10... Step: 8800... Loss: 3.5235... ppl: 33.3951  Val Loss: 3.5084\n",
            "Epoch: 4/10... Step: 8832... Loss: 3.0117... ppl: 33.3333  Val Loss: 3.5066\n",
            "Epoch: 4/10... Step: 8864... Loss: 3.2071... ppl: 33.5054  Val Loss: 3.5117\n",
            "Epoch: 4/10... Step: 8896... Loss: 2.6143... ppl: 33.2860  Val Loss: 3.5051\n",
            "Epoch: 4/10... Step: 8928... Loss: 2.8162... ppl: 33.4458  Val Loss: 3.5099\n",
            "Epoch: 4/10... Step: 8960... Loss: 2.3394... ppl: 33.1591  Val Loss: 3.5013\n",
            "Epoch: 4/10... Step: 8992... Loss: 2.9739... ppl: 33.0562  Val Loss: 3.4982\n",
            "Epoch: 4/10... Step: 9024... Loss: 3.5096... ppl: 33.4373  Val Loss: 3.5097\n",
            "Epoch: 4/10... Step: 9056... Loss: 3.0461... ppl: 33.2210  Val Loss: 3.5032\n",
            "Epoch: 4/10... Step: 9088... Loss: 3.3365... ppl: 33.2925  Val Loss: 3.5053\n",
            "Epoch: 4/10... Step: 9120... Loss: 2.6217... ppl: 33.3604  Val Loss: 3.5074\n",
            "Epoch: 4/10... Step: 9152... Loss: 2.8914... ppl: 33.3941  Val Loss: 3.5084\n",
            "Epoch: 4/10... Step: 9184... Loss: 3.3032... ppl: 33.1892  Val Loss: 3.5022\n",
            "Epoch: 4/10... Step: 9216... Loss: 2.4180... ppl: 33.2889  Val Loss: 3.5052\n",
            "Epoch: 4/10... Step: 9248... Loss: 3.5937... ppl: 33.3321  Val Loss: 3.5065\n",
            "Epoch: 4/10... Step: 9280... Loss: 2.6838... ppl: 33.9504  Val Loss: 3.5249\n",
            "Epoch: 4/10... Step: 9312... Loss: 3.3210... ppl: 33.2489  Val Loss: 3.5040\n",
            "Epoch: 4/10... Step: 9344... Loss: 3.2683... ppl: 33.1359  Val Loss: 3.5006\n",
            "Epoch: 5/10... Step: 9376... Loss: 1.9645... ppl: 33.1268  Val Loss: 3.5003\n",
            "Epoch: 5/10... Step: 9408... Loss: 3.3241... ppl: 33.4524  Val Loss: 3.5101\n",
            "Epoch: 5/10... Step: 9440... Loss: 2.8491... ppl: 33.1801  Val Loss: 3.5020\n",
            "Epoch: 5/10... Step: 9472... Loss: 2.6394... ppl: 33.3054  Val Loss: 3.5057\n",
            "Epoch: 5/10... Step: 9504... Loss: 2.9959... ppl: 33.3141  Val Loss: 3.5060\n",
            "Epoch: 5/10... Step: 9536... Loss: 2.2288... ppl: 33.2507  Val Loss: 3.5041\n",
            "Epoch: 5/10... Step: 9568... Loss: 1.9132... ppl: 33.5404  Val Loss: 3.5127\n",
            "Epoch: 5/10... Step: 9600... Loss: 3.1097... ppl: 33.3826  Val Loss: 3.5080\n",
            "Epoch: 5/10... Step: 9632... Loss: 3.0629... ppl: 33.6254  Val Loss: 3.5153\n",
            "Epoch: 5/10... Step: 9664... Loss: 3.1598... ppl: 33.7239  Val Loss: 3.5182\n",
            "Epoch: 5/10... Step: 9696... Loss: 2.3065... ppl: 33.7918  Val Loss: 3.5202\n",
            "Epoch: 5/10... Step: 9728... Loss: 3.1609... ppl: 33.4742  Val Loss: 3.5108\n",
            "Epoch: 5/10... Step: 9760... Loss: 2.4207... ppl: 33.3769  Val Loss: 3.5079\n",
            "Epoch: 5/10... Step: 9792... Loss: 3.4449... ppl: 33.5402  Val Loss: 3.5127\n",
            "Epoch: 5/10... Step: 9824... Loss: 3.2787... ppl: 33.6867  Val Loss: 3.5171\n",
            "Epoch: 5/10... Step: 9856... Loss: 2.0839... ppl: 33.9137  Val Loss: 3.5238\n",
            "Epoch: 5/10... Step: 9888... Loss: 3.1405... ppl: 33.5723  Val Loss: 3.5137\n",
            "Epoch: 5/10... Step: 9920... Loss: 2.9825... ppl: 33.7253  Val Loss: 3.5182\n",
            "Epoch: 5/10... Step: 9952... Loss: 3.2306... ppl: 33.5236  Val Loss: 3.5122\n",
            "Epoch: 5/10... Step: 9984... Loss: 3.0326... ppl: 33.5492  Val Loss: 3.5130\n",
            "Epoch: 5/10... Step: 10016... Loss: 2.7401... ppl: 33.7205  Val Loss: 3.5181\n",
            "Epoch: 5/10... Step: 10048... Loss: 2.7755... ppl: 33.6648  Val Loss: 3.5165\n",
            "Epoch: 5/10... Step: 10080... Loss: 2.8716... ppl: 33.5719  Val Loss: 3.5137\n",
            "Epoch: 5/10... Step: 10112... Loss: 3.8232... ppl: 33.6227  Val Loss: 3.5152\n",
            "Epoch: 5/10... Step: 10144... Loss: 2.7025... ppl: 33.4419  Val Loss: 3.5098\n",
            "Epoch: 5/10... Step: 10176... Loss: 2.5205... ppl: 33.5694  Val Loss: 3.5136\n",
            "Epoch: 5/10... Step: 10208... Loss: 3.2342... ppl: 33.6125  Val Loss: 3.5149\n",
            "Epoch: 5/10... Step: 10240... Loss: 3.0946... ppl: 33.6264  Val Loss: 3.5153\n",
            "Epoch: 5/10... Step: 10272... Loss: 3.6304... ppl: 33.6343  Val Loss: 3.5155\n",
            "Epoch: 5/10... Step: 10304... Loss: 3.5607... ppl: 33.5773  Val Loss: 3.5139\n",
            "Epoch: 5/10... Step: 10336... Loss: 2.8873... ppl: 33.6661  Val Loss: 3.5165\n",
            "Epoch: 5/10... Step: 10368... Loss: 2.9240... ppl: 33.5898  Val Loss: 3.5142\n",
            "Epoch: 5/10... Step: 10400... Loss: 3.0145... ppl: 33.6067  Val Loss: 3.5147\n",
            "Epoch: 5/10... Step: 10432... Loss: 2.9582... ppl: 33.7936  Val Loss: 3.5203\n",
            "Epoch: 5/10... Step: 10464... Loss: 3.0340... ppl: 33.5381  Val Loss: 3.5127\n",
            "Epoch: 5/10... Step: 10496... Loss: 2.6170... ppl: 33.5155  Val Loss: 3.5120\n",
            "Epoch: 5/10... Step: 10528... Loss: 3.1670... ppl: 33.5094  Val Loss: 3.5118\n",
            "Epoch: 5/10... Step: 10560... Loss: 3.5630... ppl: 33.3276  Val Loss: 3.5064\n",
            "Epoch: 5/10... Step: 10592... Loss: 2.9895... ppl: 33.4310  Val Loss: 3.5095\n",
            "Epoch: 5/10... Step: 10624... Loss: 2.9537... ppl: 33.4505  Val Loss: 3.5101\n",
            "Epoch: 5/10... Step: 10656... Loss: 2.9327... ppl: 33.2503  Val Loss: 3.5041\n",
            "Epoch: 5/10... Step: 10688... Loss: 2.7802... ppl: 33.2836  Val Loss: 3.5051\n",
            "Epoch: 5/10... Step: 10720... Loss: 2.8323... ppl: 33.6161  Val Loss: 3.5150\n",
            "Epoch: 5/10... Step: 10752... Loss: 2.6006... ppl: 33.6333  Val Loss: 3.5155\n",
            "Epoch: 5/10... Step: 10784... Loss: 3.5362... ppl: 33.4094  Val Loss: 3.5088\n",
            "Epoch: 5/10... Step: 10816... Loss: 3.0596... ppl: 33.5431  Val Loss: 3.5128\n",
            "Epoch: 5/10... Step: 10848... Loss: 3.4196... ppl: 33.3840  Val Loss: 3.5081\n",
            "Epoch: 5/10... Step: 10880... Loss: 2.8207... ppl: 33.5326  Val Loss: 3.5125\n",
            "Epoch: 5/10... Step: 10912... Loss: 2.3417... ppl: 33.4681  Val Loss: 3.5106\n",
            "Epoch: 5/10... Step: 10944... Loss: 3.0561... ppl: 33.5013  Val Loss: 3.5116\n",
            "Epoch: 5/10... Step: 10976... Loss: 3.1632... ppl: 33.4763  Val Loss: 3.5108\n",
            "Epoch: 5/10... Step: 11008... Loss: 3.1691... ppl: 33.3343  Val Loss: 3.5066\n",
            "Epoch: 5/10... Step: 11040... Loss: 3.5004... ppl: 33.2412  Val Loss: 3.5038\n",
            "Epoch: 5/10... Step: 11072... Loss: 3.6149... ppl: 33.4718  Val Loss: 3.5107\n",
            "Epoch: 5/10... Step: 11104... Loss: 3.0652... ppl: 33.4137  Val Loss: 3.5090\n",
            "Epoch: 5/10... Step: 11136... Loss: 3.6521... ppl: 33.5014  Val Loss: 3.5116\n",
            "Epoch: 5/10... Step: 11168... Loss: 2.4722... ppl: 33.5641  Val Loss: 3.5135\n",
            "Epoch: 5/10... Step: 11200... Loss: 2.9057... ppl: 33.8059  Val Loss: 3.5206\n",
            "Epoch: 5/10... Step: 11232... Loss: 2.6289... ppl: 33.4716  Val Loss: 3.5107\n",
            "Epoch: 5/10... Step: 11264... Loss: 2.0927... ppl: 33.5835  Val Loss: 3.5140\n",
            "Epoch: 5/10... Step: 11296... Loss: 2.6360... ppl: 33.6011  Val Loss: 3.5146\n",
            "Epoch: 5/10... Step: 11328... Loss: 3.5427... ppl: 33.2449  Val Loss: 3.5039\n",
            "Epoch: 5/10... Step: 11360... Loss: 2.7445... ppl: 33.6774  Val Loss: 3.5168\n",
            "Epoch: 5/10... Step: 11392... Loss: 2.6405... ppl: 33.5110  Val Loss: 3.5119\n",
            "Epoch: 5/10... Step: 11424... Loss: 3.4939... ppl: 33.4576  Val Loss: 3.5103\n",
            "Epoch: 5/10... Step: 11456... Loss: 2.2683... ppl: 33.5606  Val Loss: 3.5134\n",
            "Epoch: 5/10... Step: 11488... Loss: 2.6559... ppl: 33.6568  Val Loss: 3.5162\n",
            "Epoch: 5/10... Step: 11520... Loss: 2.2786... ppl: 33.4658  Val Loss: 3.5105\n",
            "Epoch: 5/10... Step: 11552... Loss: 3.7017... ppl: 33.5787  Val Loss: 3.5139\n",
            "Epoch: 5/10... Step: 11584... Loss: 2.9754... ppl: 33.4804  Val Loss: 3.5110\n",
            "Epoch: 5/10... Step: 11616... Loss: 2.3592... ppl: 34.1319  Val Loss: 3.5302\n",
            "Epoch: 5/10... Step: 11648... Loss: 2.8665... ppl: 33.7096  Val Loss: 3.5178\n",
            "Epoch: 5/10... Step: 11680... Loss: 3.2556... ppl: 33.4668  Val Loss: 3.5106\n",
            "Epoch: 5/10... Step: 11712... Loss: 3.0707... ppl: 33.3934  Val Loss: 3.5084\n",
            "Epoch: 6/10... Step: 11744... Loss: 3.4894... ppl: 33.5401  Val Loss: 3.5127\n",
            "Epoch: 6/10... Step: 11776... Loss: 2.8734... ppl: 33.5218  Val Loss: 3.5122\n",
            "Epoch: 6/10... Step: 11808... Loss: 2.5542... ppl: 33.5510  Val Loss: 3.5131\n",
            "Epoch: 6/10... Step: 11840... Loss: 3.3674... ppl: 33.6676  Val Loss: 3.5165\n",
            "Epoch: 6/10... Step: 11872... Loss: 2.8494... ppl: 33.5075  Val Loss: 3.5118\n",
            "Epoch: 6/10... Step: 11904... Loss: 2.9724... ppl: 33.7768  Val Loss: 3.5198\n",
            "Epoch: 6/10... Step: 11936... Loss: 2.9348... ppl: 33.6930  Val Loss: 3.5173\n",
            "Epoch: 6/10... Step: 11968... Loss: 2.8064... ppl: 33.9414  Val Loss: 3.5246\n",
            "Epoch: 6/10... Step: 12000... Loss: 2.5854... ppl: 33.9039  Val Loss: 3.5235\n",
            "Epoch: 6/10... Step: 12032... Loss: 3.0121... ppl: 34.2190  Val Loss: 3.5328\n",
            "Epoch: 6/10... Step: 12064... Loss: 3.1004... ppl: 33.8959  Val Loss: 3.5233\n",
            "Epoch: 6/10... Step: 12096... Loss: 3.7240... ppl: 33.7096  Val Loss: 3.5178\n",
            "Epoch: 6/10... Step: 12128... Loss: 2.5174... ppl: 33.8285  Val Loss: 3.5213\n",
            "Epoch: 6/10... Step: 12160... Loss: 2.9309... ppl: 34.0410  Val Loss: 3.5276\n",
            "Epoch: 6/10... Step: 12192... Loss: 2.8022... ppl: 34.2792  Val Loss: 3.5345\n",
            "Epoch: 6/10... Step: 12224... Loss: 2.5046... ppl: 34.0948  Val Loss: 3.5291\n",
            "Epoch: 6/10... Step: 12256... Loss: 2.6093... ppl: 34.2345  Val Loss: 3.5332\n",
            "Epoch: 6/10... Step: 12288... Loss: 2.8014... ppl: 34.0471  Val Loss: 3.5277\n",
            "Epoch: 6/10... Step: 12320... Loss: 2.9270... ppl: 34.0813  Val Loss: 3.5287\n",
            "Epoch: 6/10... Step: 12352... Loss: 2.6808... ppl: 34.1723  Val Loss: 3.5314\n",
            "Epoch: 6/10... Step: 12384... Loss: 3.0373... ppl: 34.0489  Val Loss: 3.5278\n",
            "Epoch: 6/10... Step: 12416... Loss: 2.9899... ppl: 34.1052  Val Loss: 3.5295\n",
            "Epoch: 6/10... Step: 12448... Loss: 2.9505... ppl: 33.9937  Val Loss: 3.5262\n",
            "Epoch: 6/10... Step: 12480... Loss: 2.5661... ppl: 33.9465  Val Loss: 3.5248\n",
            "Epoch: 6/10... Step: 12512... Loss: 2.8609... ppl: 33.9915  Val Loss: 3.5261\n",
            "Epoch: 6/10... Step: 12544... Loss: 2.9729... ppl: 34.0791  Val Loss: 3.5287\n",
            "Epoch: 6/10... Step: 12576... Loss: 2.4502... ppl: 34.2729  Val Loss: 3.5344\n",
            "Epoch: 6/10... Step: 12608... Loss: 3.2222... ppl: 34.0507  Val Loss: 3.5278\n",
            "Epoch: 6/10... Step: 12640... Loss: 2.6108... ppl: 34.0718  Val Loss: 3.5285\n",
            "Epoch: 6/10... Step: 12672... Loss: 3.2275... ppl: 34.2129  Val Loss: 3.5326\n",
            "Epoch: 6/10... Step: 12704... Loss: 2.4879... ppl: 34.0607  Val Loss: 3.5281\n",
            "Epoch: 6/10... Step: 12736... Loss: 3.1421... ppl: 33.9406  Val Loss: 3.5246\n",
            "Epoch: 6/10... Step: 12768... Loss: 2.5494... ppl: 34.2920  Val Loss: 3.5349\n",
            "Epoch: 6/10... Step: 12800... Loss: 2.9573... ppl: 34.1133  Val Loss: 3.5297\n",
            "Epoch: 6/10... Step: 12832... Loss: 2.9232... ppl: 34.0645  Val Loss: 3.5283\n",
            "Epoch: 6/10... Step: 12864... Loss: 2.9043... ppl: 34.0388  Val Loss: 3.5275\n",
            "Epoch: 6/10... Step: 12896... Loss: 3.2563... ppl: 33.6911  Val Loss: 3.5172\n",
            "Epoch: 6/10... Step: 12928... Loss: 2.7505... ppl: 33.8743  Val Loss: 3.5227\n",
            "Epoch: 6/10... Step: 12960... Loss: 3.0348... ppl: 33.9084  Val Loss: 3.5237\n",
            "Epoch: 6/10... Step: 12992... Loss: 3.1145... ppl: 33.8095  Val Loss: 3.5207\n",
            "Epoch: 6/10... Step: 13024... Loss: 2.4528... ppl: 33.7929  Val Loss: 3.5202\n",
            "Epoch: 6/10... Step: 13056... Loss: 2.6541... ppl: 33.9928  Val Loss: 3.5261\n",
            "Epoch: 6/10... Step: 13088... Loss: 2.6317... ppl: 34.2701  Val Loss: 3.5343\n",
            "Epoch: 6/10... Step: 13120... Loss: 3.3025... ppl: 33.9724  Val Loss: 3.5255\n",
            "Epoch: 6/10... Step: 13152... Loss: 2.7120... ppl: 34.1347  Val Loss: 3.5303\n",
            "Epoch: 6/10... Step: 13184... Loss: 3.1863... ppl: 33.9608  Val Loss: 3.5252\n",
            "Epoch: 6/10... Step: 13216... Loss: 3.4818... ppl: 33.9545  Val Loss: 3.5250\n",
            "Epoch: 6/10... Step: 13248... Loss: 3.4134... ppl: 34.0248  Val Loss: 3.5271\n",
            "Epoch: 6/10... Step: 13280... Loss: 3.3016... ppl: 34.0029  Val Loss: 3.5264\n",
            "Epoch: 6/10... Step: 13312... Loss: 3.0001... ppl: 34.1379  Val Loss: 3.5304\n",
            "Epoch: 6/10... Step: 13344... Loss: 2.4932... ppl: 33.9385  Val Loss: 3.5246\n",
            "Epoch: 6/10... Step: 13376... Loss: 2.5484... ppl: 33.8876  Val Loss: 3.5230\n",
            "Epoch: 6/10... Step: 13408... Loss: 2.1887... ppl: 34.1236  Val Loss: 3.5300\n",
            "Epoch: 6/10... Step: 13440... Loss: 3.2568... ppl: 34.0427  Val Loss: 3.5276\n",
            "Epoch: 6/10... Step: 13472... Loss: 3.1557... ppl: 34.0217  Val Loss: 3.5270\n",
            "Epoch: 6/10... Step: 13504... Loss: 2.8501... ppl: 34.2579  Val Loss: 3.5339\n",
            "Epoch: 6/10... Step: 13536... Loss: 3.0324... ppl: 34.3231  Val Loss: 3.5358\n",
            "Epoch: 6/10... Step: 13568... Loss: 2.7647... ppl: 34.1035  Val Loss: 3.5294\n",
            "Epoch: 6/10... Step: 13600... Loss: 2.7819... ppl: 34.0897  Val Loss: 3.5290\n",
            "Epoch: 6/10... Step: 13632... Loss: 3.6079... ppl: 34.3947  Val Loss: 3.5379\n",
            "Epoch: 6/10... Step: 13664... Loss: 2.7156... ppl: 33.9834  Val Loss: 3.5259\n",
            "Epoch: 6/10... Step: 13696... Loss: 3.2627... ppl: 34.1790  Val Loss: 3.5316\n",
            "Epoch: 6/10... Step: 13728... Loss: 2.4452... ppl: 34.2366  Val Loss: 3.5333\n",
            "Epoch: 6/10... Step: 13760... Loss: 3.2139... ppl: 34.1022  Val Loss: 3.5294\n",
            "Epoch: 6/10... Step: 13792... Loss: 3.1092... ppl: 34.2100  Val Loss: 3.5325\n",
            "Epoch: 6/10... Step: 13824... Loss: 2.5139... ppl: 34.3332  Val Loss: 3.5361\n",
            "Epoch: 6/10... Step: 13856... Loss: 3.2250... ppl: 34.3073  Val Loss: 3.5354\n",
            "Epoch: 6/10... Step: 13888... Loss: 2.8791... ppl: 34.1126  Val Loss: 3.5297\n",
            "Epoch: 6/10... Step: 13920... Loss: 3.2966... ppl: 34.1699  Val Loss: 3.5313\n",
            "Epoch: 6/10... Step: 13952... Loss: 2.1751... ppl: 34.5868  Val Loss: 3.5435\n",
            "Epoch: 6/10... Step: 13984... Loss: 2.9844... ppl: 34.6894  Val Loss: 3.5464\n",
            "Epoch: 6/10... Step: 14016... Loss: 2.5170... ppl: 34.2329  Val Loss: 3.5332\n",
            "Epoch: 6/10... Step: 14048... Loss: 2.4274... ppl: 34.0664  Val Loss: 3.5283\n",
            "Epoch: 7/10... Step: 14080... Loss: 3.0487... ppl: 34.0485  Val Loss: 3.5278\n",
            "Epoch: 7/10... Step: 14112... Loss: 2.0252... ppl: 34.4346  Val Loss: 3.5391\n",
            "Epoch: 7/10... Step: 14144... Loss: 2.6974... ppl: 34.2233  Val Loss: 3.5329\n",
            "Epoch: 7/10... Step: 14176... Loss: 2.6436... ppl: 34.4112  Val Loss: 3.5384\n",
            "Epoch: 7/10... Step: 14208... Loss: 2.4307... ppl: 34.2905  Val Loss: 3.5349\n",
            "Epoch: 7/10... Step: 14240... Loss: 2.8183... ppl: 34.3732  Val Loss: 3.5373\n",
            "Epoch: 7/10... Step: 14272... Loss: 2.4433... ppl: 34.4688  Val Loss: 3.5401\n",
            "Epoch: 7/10... Step: 14304... Loss: 2.3448... ppl: 34.5352  Val Loss: 3.5420\n",
            "Epoch: 7/10... Step: 14336... Loss: 2.6828... ppl: 34.5923  Val Loss: 3.5436\n",
            "Epoch: 7/10... Step: 14368... Loss: 3.4450... ppl: 35.0495  Val Loss: 3.5568\n",
            "Epoch: 7/10... Step: 14400... Loss: 2.5692... ppl: 34.6570  Val Loss: 3.5455\n",
            "Epoch: 7/10... Step: 14432... Loss: 2.8747... ppl: 34.3872  Val Loss: 3.5377\n",
            "Epoch: 7/10... Step: 14464... Loss: 3.0499... ppl: 34.5264  Val Loss: 3.5417\n",
            "Epoch: 7/10... Step: 14496... Loss: 3.1288... ppl: 34.7966  Val Loss: 3.5495\n",
            "Epoch: 7/10... Step: 14528... Loss: 2.1090... ppl: 34.9127  Val Loss: 3.5529\n",
            "Epoch: 7/10... Step: 14560... Loss: 2.3832... ppl: 34.9529  Val Loss: 3.5540\n",
            "Epoch: 7/10... Step: 14592... Loss: 2.5526... ppl: 34.8150  Val Loss: 3.5500\n",
            "Epoch: 7/10... Step: 14624... Loss: 2.1684... ppl: 34.8188  Val Loss: 3.5502\n",
            "Epoch: 7/10... Step: 14656... Loss: 2.9689... ppl: 34.8986  Val Loss: 3.5524\n",
            "Epoch: 7/10... Step: 14688... Loss: 2.9705... ppl: 34.9994  Val Loss: 3.5553\n",
            "Epoch: 7/10... Step: 14720... Loss: 3.4268... ppl: 34.8373  Val Loss: 3.5507\n",
            "Epoch: 7/10... Step: 14752... Loss: 2.9769... ppl: 34.9345  Val Loss: 3.5535\n",
            "Epoch: 7/10... Step: 14784... Loss: 2.1129... ppl: 34.8582  Val Loss: 3.5513\n",
            "Epoch: 7/10... Step: 14816... Loss: 2.6760... ppl: 34.7712  Val Loss: 3.5488\n",
            "Epoch: 7/10... Step: 14848... Loss: 2.6112... ppl: 34.5958  Val Loss: 3.5437\n",
            "Epoch: 7/10... Step: 14880... Loss: 2.6707... ppl: 34.9104  Val Loss: 3.5528\n",
            "Epoch: 7/10... Step: 14912... Loss: 2.5247... ppl: 35.0375  Val Loss: 3.5564\n",
            "Epoch: 7/10... Step: 14944... Loss: 3.0330... ppl: 34.8745  Val Loss: 3.5518\n",
            "Epoch: 7/10... Step: 14976... Loss: 2.6674... ppl: 34.7690  Val Loss: 3.5487\n",
            "Epoch: 7/10... Step: 15008... Loss: 2.8431... ppl: 34.9775  Val Loss: 3.5547\n",
            "Epoch: 7/10... Step: 15040... Loss: 2.1424... ppl: 34.8780  Val Loss: 3.5519\n",
            "Epoch: 7/10... Step: 15072... Loss: 3.0686... ppl: 34.7360  Val Loss: 3.5478\n",
            "Epoch: 7/10... Step: 15104... Loss: 2.8452... ppl: 35.1464  Val Loss: 3.5595\n",
            "Epoch: 7/10... Step: 15136... Loss: 3.1751... ppl: 35.1659  Val Loss: 3.5601\n",
            "Epoch: 7/10... Step: 15168... Loss: 2.3774... ppl: 34.9119  Val Loss: 3.5528\n",
            "Epoch: 7/10... Step: 15200... Loss: 2.6933... ppl: 34.8274  Val Loss: 3.5504\n",
            "Epoch: 7/10... Step: 15232... Loss: 3.1532... ppl: 34.4669  Val Loss: 3.5400\n",
            "Epoch: 7/10... Step: 15264... Loss: 2.7394... ppl: 34.7434  Val Loss: 3.5480\n",
            "Epoch: 7/10... Step: 15296... Loss: 2.9419... ppl: 34.7005  Val Loss: 3.5468\n",
            "Epoch: 7/10... Step: 15328... Loss: 2.6987... ppl: 34.6684  Val Loss: 3.5458\n",
            "Epoch: 7/10... Step: 15360... Loss: 2.8239... ppl: 34.7509  Val Loss: 3.5482\n",
            "Epoch: 7/10... Step: 15392... Loss: 2.8207... ppl: 34.6749  Val Loss: 3.5460\n",
            "Epoch: 7/10... Step: 15424... Loss: 2.2983... ppl: 35.1184  Val Loss: 3.5587\n",
            "Epoch: 7/10... Step: 15456... Loss: 3.1594... ppl: 34.8198  Val Loss: 3.5502\n",
            "Epoch: 7/10... Step: 15488... Loss: 2.8772... ppl: 34.9557  Val Loss: 3.5541\n",
            "Epoch: 7/10... Step: 15520... Loss: 3.0685... ppl: 34.8933  Val Loss: 3.5523\n",
            "Epoch: 7/10... Step: 15552... Loss: 2.5754... ppl: 34.7171  Val Loss: 3.5472\n",
            "Epoch: 7/10... Step: 15584... Loss: 3.3606... ppl: 34.8630  Val Loss: 3.5514\n",
            "Epoch: 7/10... Step: 15616... Loss: 2.1811... ppl: 34.8121  Val Loss: 3.5500\n",
            "Epoch: 7/10... Step: 15648... Loss: 2.8673... ppl: 34.9966  Val Loss: 3.5553\n",
            "Epoch: 7/10... Step: 15680... Loss: 2.7602... ppl: 34.8143  Val Loss: 3.5500\n",
            "Epoch: 7/10... Step: 15712... Loss: 2.1869... ppl: 34.7066  Val Loss: 3.5469\n",
            "Epoch: 7/10... Step: 15744... Loss: 2.8164... ppl: 34.8690  Val Loss: 3.5516\n",
            "Epoch: 7/10... Step: 15776... Loss: 2.8844... ppl: 35.0163  Val Loss: 3.5558\n",
            "Epoch: 7/10... Step: 15808... Loss: 3.1319... ppl: 34.9292  Val Loss: 3.5533\n",
            "Epoch: 7/10... Step: 15840... Loss: 3.4794... ppl: 35.1328  Val Loss: 3.5591\n",
            "Epoch: 7/10... Step: 15872... Loss: 3.3669... ppl: 35.0828  Val Loss: 3.5577\n",
            "Epoch: 7/10... Step: 15904... Loss: 2.4426... ppl: 35.1664  Val Loss: 3.5601\n",
            "Epoch: 7/10... Step: 15936... Loss: 2.5986... ppl: 34.9540  Val Loss: 3.5540\n",
            "Epoch: 7/10... Step: 15968... Loss: 1.9470... ppl: 35.4183  Val Loss: 3.5672\n",
            "Epoch: 7/10... Step: 16000... Loss: 2.8587... ppl: 34.9372  Val Loss: 3.5536\n",
            "Epoch: 7/10... Step: 16032... Loss: 2.6877... ppl: 34.9489  Val Loss: 3.5539\n",
            "Epoch: 7/10... Step: 16064... Loss: 2.2655... ppl: 35.2071  Val Loss: 3.5612\n",
            "Epoch: 7/10... Step: 16096... Loss: 2.1953... ppl: 34.9465  Val Loss: 3.5538\n",
            "Epoch: 7/10... Step: 16128... Loss: 2.4489... ppl: 34.9381  Val Loss: 3.5536\n",
            "Epoch: 7/10... Step: 16160... Loss: 2.6948... ppl: 35.1799  Val Loss: 3.5605\n",
            "Epoch: 7/10... Step: 16192... Loss: 2.5523... ppl: 35.2594  Val Loss: 3.5627\n",
            "Epoch: 7/10... Step: 16224... Loss: 3.0286... ppl: 34.9702  Val Loss: 3.5545\n",
            "Epoch: 7/10... Step: 16256... Loss: 2.4585... ppl: 35.0999  Val Loss: 3.5582\n",
            "Epoch: 7/10... Step: 16288... Loss: 2.7813... ppl: 35.1755  Val Loss: 3.5604\n",
            "Epoch: 7/10... Step: 16320... Loss: 2.4059... ppl: 35.7202  Val Loss: 3.5757\n",
            "Epoch: 7/10... Step: 16352... Loss: 2.5613... ppl: 35.2418  Val Loss: 3.5622\n",
            "Epoch: 7/10... Step: 16384... Loss: 3.4236... ppl: 34.9508  Val Loss: 3.5539\n",
            "Epoch: 8/10... Step: 16416... Loss: 3.2151... ppl: 34.8465  Val Loss: 3.5510\n",
            "Epoch: 8/10... Step: 16448... Loss: 2.6895... ppl: 35.3578  Val Loss: 3.5655\n",
            "Epoch: 8/10... Step: 16480... Loss: 2.7375... ppl: 35.1485  Val Loss: 3.5596\n",
            "Epoch: 8/10... Step: 16512... Loss: 1.7403... ppl: 35.1795  Val Loss: 3.5605\n",
            "Epoch: 8/10... Step: 16544... Loss: 2.6593... ppl: 35.1922  Val Loss: 3.5608\n",
            "Epoch: 8/10... Step: 16576... Loss: 2.9639... ppl: 35.2277  Val Loss: 3.5618\n",
            "Epoch: 8/10... Step: 16608... Loss: 2.7733... ppl: 35.3643  Val Loss: 3.5657\n",
            "Epoch: 8/10... Step: 16640... Loss: 2.4580... ppl: 35.3232  Val Loss: 3.5645\n",
            "Epoch: 8/10... Step: 16672... Loss: 2.7207... ppl: 35.4541  Val Loss: 3.5682\n",
            "Epoch: 8/10... Step: 16704... Loss: 2.7684... ppl: 35.8204  Val Loss: 3.5785\n",
            "Epoch: 8/10... Step: 16736... Loss: 2.9413... ppl: 35.6850  Val Loss: 3.5747\n",
            "Epoch: 8/10... Step: 16768... Loss: 2.8455... ppl: 35.3043  Val Loss: 3.5640\n",
            "Epoch: 8/10... Step: 16800... Loss: 2.2326... ppl: 35.1878  Val Loss: 3.5607\n",
            "Epoch: 8/10... Step: 16832... Loss: 2.4167... ppl: 35.4655  Val Loss: 3.5686\n",
            "Epoch: 8/10... Step: 16864... Loss: 2.6230... ppl: 35.7683  Val Loss: 3.5771\n",
            "Epoch: 8/10... Step: 16896... Loss: 2.3394... ppl: 35.8375  Val Loss: 3.5790\n",
            "Epoch: 8/10... Step: 16928... Loss: 2.4203... ppl: 35.6592  Val Loss: 3.5740\n",
            "Epoch: 8/10... Step: 16960... Loss: 2.4232... ppl: 35.7464  Val Loss: 3.5765\n",
            "Epoch: 8/10... Step: 16992... Loss: 2.5442... ppl: 35.7628  Val Loss: 3.5769\n",
            "Epoch: 8/10... Step: 17024... Loss: 2.2590... ppl: 35.6983  Val Loss: 3.5751\n",
            "Epoch: 8/10... Step: 17056... Loss: 2.6002... ppl: 35.8094  Val Loss: 3.5782\n",
            "Epoch: 8/10... Step: 17088... Loss: 3.1816... ppl: 35.7557  Val Loss: 3.5767\n",
            "Epoch: 8/10... Step: 17120... Loss: 2.7073... ppl: 35.7327  Val Loss: 3.5761\n",
            "Epoch: 8/10... Step: 17152... Loss: 2.9628... ppl: 35.7449  Val Loss: 3.5764\n",
            "Epoch: 8/10... Step: 17184... Loss: 2.5702... ppl: 35.4340  Val Loss: 3.5677\n",
            "Epoch: 8/10... Step: 17216... Loss: 2.3878... ppl: 35.7975  Val Loss: 3.5779\n",
            "Epoch: 8/10... Step: 17248... Loss: 2.3301... ppl: 35.9452  Val Loss: 3.5820\n",
            "Epoch: 8/10... Step: 17280... Loss: 2.3197... ppl: 35.8436  Val Loss: 3.5792\n",
            "Epoch: 8/10... Step: 17312... Loss: 3.4592... ppl: 35.7087  Val Loss: 3.5754\n",
            "Epoch: 8/10... Step: 17344... Loss: 2.4848... ppl: 36.0697  Val Loss: 3.5855\n",
            "Epoch: 8/10... Step: 17376... Loss: 2.7058... ppl: 36.0593  Val Loss: 3.5852\n",
            "Epoch: 8/10... Step: 17408... Loss: 2.6268... ppl: 35.9662  Val Loss: 3.5826\n",
            "Epoch: 8/10... Step: 17440... Loss: 2.6602... ppl: 36.0122  Val Loss: 3.5839\n",
            "Epoch: 8/10... Step: 17472... Loss: 2.6362... ppl: 36.2112  Val Loss: 3.5894\n",
            "Epoch: 8/10... Step: 17504... Loss: 2.6514... ppl: 35.8580  Val Loss: 3.5796\n",
            "Epoch: 8/10... Step: 17536... Loss: 2.7060... ppl: 35.8681  Val Loss: 3.5798\n",
            "Epoch: 8/10... Step: 17568... Loss: 2.9594... ppl: 35.7539  Val Loss: 3.5767\n",
            "Epoch: 8/10... Step: 17600... Loss: 2.6050... ppl: 35.5736  Val Loss: 3.5716\n",
            "Epoch: 8/10... Step: 17632... Loss: 2.5521... ppl: 35.5497  Val Loss: 3.5709\n",
            "Epoch: 8/10... Step: 17664... Loss: 2.6061... ppl: 35.6982  Val Loss: 3.5751\n",
            "Epoch: 8/10... Step: 17696... Loss: 3.0395... ppl: 35.6335  Val Loss: 3.5733\n",
            "Epoch: 8/10... Step: 17728... Loss: 3.1009... ppl: 35.5485  Val Loss: 3.5709\n",
            "Epoch: 8/10... Step: 17760... Loss: 2.4853... ppl: 36.1120  Val Loss: 3.5866\n",
            "Epoch: 8/10... Step: 17792... Loss: 2.5297... ppl: 36.0197  Val Loss: 3.5841\n",
            "Epoch: 8/10... Step: 17824... Loss: 2.8190... ppl: 35.8491  Val Loss: 3.5793\n",
            "Epoch: 8/10... Step: 17856... Loss: 2.4906... ppl: 35.9636  Val Loss: 3.5825\n",
            "Epoch: 8/10... Step: 17888... Loss: 2.7770... ppl: 35.7038  Val Loss: 3.5753\n",
            "Epoch: 8/10... Step: 17920... Loss: 2.1747... ppl: 35.8020  Val Loss: 3.5780\n",
            "Epoch: 8/10... Step: 17952... Loss: 2.6842... ppl: 35.7995  Val Loss: 3.5779\n",
            "Epoch: 8/10... Step: 17984... Loss: 2.7121... ppl: 35.9128  Val Loss: 3.5811\n",
            "Epoch: 8/10... Step: 18016... Loss: 2.8615... ppl: 35.8538  Val Loss: 3.5794\n",
            "Epoch: 8/10... Step: 18048... Loss: 2.9609... ppl: 35.6818  Val Loss: 3.5746\n",
            "Epoch: 8/10... Step: 18080... Loss: 2.9710... ppl: 35.8195  Val Loss: 3.5785\n",
            "Epoch: 8/10... Step: 18112... Loss: 2.2531... ppl: 36.0205  Val Loss: 3.5841\n",
            "Epoch: 8/10... Step: 18144... Loss: 3.4050... ppl: 35.9135  Val Loss: 3.5811\n",
            "Epoch: 8/10... Step: 18176... Loss: 2.7825... ppl: 36.1133  Val Loss: 3.5867\n",
            "Epoch: 8/10... Step: 18208... Loss: 2.7699... ppl: 36.0452  Val Loss: 3.5848\n",
            "Epoch: 8/10... Step: 18240... Loss: 2.9101... ppl: 36.1704  Val Loss: 3.5882\n",
            "Epoch: 8/10... Step: 18272... Loss: 2.0214... ppl: 35.9818  Val Loss: 3.5830\n",
            "Epoch: 8/10... Step: 18304... Loss: 2.8255... ppl: 36.2909  Val Loss: 3.5916\n",
            "Epoch: 8/10... Step: 18336... Loss: 3.0844... ppl: 36.0639  Val Loss: 3.5853\n",
            "Epoch: 8/10... Step: 18368... Loss: 2.8884... ppl: 35.8141  Val Loss: 3.5783\n",
            "Epoch: 8/10... Step: 18400... Loss: 2.7621... ppl: 36.2514  Val Loss: 3.5905\n",
            "Epoch: 8/10... Step: 18432... Loss: 3.0731... ppl: 36.0520  Val Loss: 3.5850\n",
            "Epoch: 8/10... Step: 18464... Loss: 2.3844... ppl: 35.9380  Val Loss: 3.5818\n",
            "Epoch: 8/10... Step: 18496... Loss: 2.1195... ppl: 36.1532  Val Loss: 3.5878\n",
            "Epoch: 8/10... Step: 18528... Loss: 2.8410... ppl: 36.2009  Val Loss: 3.5891\n",
            "Epoch: 8/10... Step: 18560... Loss: 2.3621... ppl: 35.9390  Val Loss: 3.5818\n",
            "Epoch: 8/10... Step: 18592... Loss: 2.7028... ppl: 36.2613  Val Loss: 3.5908\n",
            "Epoch: 8/10... Step: 18624... Loss: 2.7174... ppl: 36.0439  Val Loss: 3.5847\n",
            "Epoch: 8/10... Step: 18656... Loss: 3.5941... ppl: 36.8382  Val Loss: 3.6065\n",
            "Epoch: 8/10... Step: 18688... Loss: 2.1908... ppl: 36.5289  Val Loss: 3.5981\n",
            "Epoch: 8/10... Step: 18720... Loss: 2.4738... ppl: 36.0527  Val Loss: 3.5850\n",
            "Epoch: 9/10... Step: 18752... Loss: 2.4682... ppl: 35.7922  Val Loss: 3.5777\n",
            "Epoch: 9/10... Step: 18784... Loss: 2.5722... ppl: 36.1603  Val Loss: 3.5880\n",
            "Epoch: 9/10... Step: 18816... Loss: 2.8893... ppl: 36.1255  Val Loss: 3.5870\n",
            "Epoch: 9/10... Step: 18848... Loss: 2.8214... ppl: 36.0405  Val Loss: 3.5846\n",
            "Epoch: 9/10... Step: 18880... Loss: 2.8699... ppl: 36.3627  Val Loss: 3.5935\n",
            "Epoch: 9/10... Step: 18912... Loss: 2.7862... ppl: 36.2020  Val Loss: 3.5891\n",
            "Epoch: 9/10... Step: 18944... Loss: 2.6838... ppl: 36.4536  Val Loss: 3.5960\n",
            "Epoch: 9/10... Step: 18976... Loss: 2.7556... ppl: 36.3567  Val Loss: 3.5934\n",
            "Epoch: 9/10... Step: 19008... Loss: 2.9891... ppl: 36.5595  Val Loss: 3.5989\n",
            "Epoch: 9/10... Step: 19040... Loss: 2.3973... ppl: 36.7031  Val Loss: 3.6029\n",
            "Epoch: 9/10... Step: 19072... Loss: 2.5507... ppl: 36.8301  Val Loss: 3.6063\n",
            "Epoch: 9/10... Step: 19104... Loss: 2.6053... ppl: 36.4507  Val Loss: 3.5960\n",
            "Epoch: 9/10... Step: 19136... Loss: 1.8755... ppl: 36.2026  Val Loss: 3.5891\n",
            "Epoch: 9/10... Step: 19168... Loss: 2.2561... ppl: 36.2961  Val Loss: 3.5917\n",
            "Epoch: 9/10... Step: 19200... Loss: 2.9935... ppl: 36.7114  Val Loss: 3.6031\n",
            "Epoch: 9/10... Step: 19232... Loss: 2.8228... ppl: 36.9190  Val Loss: 3.6087\n",
            "Epoch: 9/10... Step: 19264... Loss: 2.8328... ppl: 36.7413  Val Loss: 3.6039\n",
            "Epoch: 9/10... Step: 19296... Loss: 3.1115... ppl: 36.7255  Val Loss: 3.6035\n",
            "Epoch: 9/10... Step: 19328... Loss: 2.8030... ppl: 36.7318  Val Loss: 3.6036\n",
            "Epoch: 9/10... Step: 19360... Loss: 2.9261... ppl: 36.6901  Val Loss: 3.6025\n",
            "Epoch: 9/10... Step: 19392... Loss: 2.4301... ppl: 36.8540  Val Loss: 3.6070\n",
            "Epoch: 9/10... Step: 19424... Loss: 2.7597... ppl: 36.7349  Val Loss: 3.6037\n",
            "Epoch: 9/10... Step: 19456... Loss: 2.6685... ppl: 36.7868  Val Loss: 3.6051\n",
            "Epoch: 9/10... Step: 19488... Loss: 2.5328... ppl: 36.8181  Val Loss: 3.6060\n",
            "Epoch: 9/10... Step: 19520... Loss: 2.6518... ppl: 36.4679  Val Loss: 3.5964\n",
            "Epoch: 9/10... Step: 19552... Loss: 2.1882... ppl: 36.8821  Val Loss: 3.6077\n",
            "Epoch: 9/10... Step: 19584... Loss: 2.2274... ppl: 36.9986  Val Loss: 3.6109\n",
            "Epoch: 9/10... Step: 19616... Loss: 2.7328... ppl: 37.0109  Val Loss: 3.6112\n",
            "Epoch: 9/10... Step: 19648... Loss: 2.8571... ppl: 36.7109  Val Loss: 3.6031\n",
            "Epoch: 9/10... Step: 19680... Loss: 2.8250... ppl: 36.9501  Val Loss: 3.6096\n",
            "Epoch: 9/10... Step: 19712... Loss: 2.2693... ppl: 37.0998  Val Loss: 3.6136\n",
            "Epoch: 9/10... Step: 19744... Loss: 1.9555... ppl: 37.0518  Val Loss: 3.6123\n",
            "Epoch: 9/10... Step: 19776... Loss: 2.9618... ppl: 37.0502  Val Loss: 3.6123\n",
            "Epoch: 9/10... Step: 19808... Loss: 2.1007... ppl: 37.4564  Val Loss: 3.6232\n",
            "Epoch: 9/10... Step: 19840... Loss: 2.3709... ppl: 36.9453  Val Loss: 3.6094\n",
            "Epoch: 9/10... Step: 19872... Loss: 2.8175... ppl: 36.9160  Val Loss: 3.6086\n",
            "Epoch: 9/10... Step: 19904... Loss: 2.4689... ppl: 36.7965  Val Loss: 3.6054\n",
            "Epoch: 9/10... Step: 19936... Loss: 2.4851... ppl: 36.5190  Val Loss: 3.5978\n",
            "Epoch: 9/10... Step: 19968... Loss: 2.5064... ppl: 36.5818  Val Loss: 3.5996\n",
            "Epoch: 9/10... Step: 20000... Loss: 3.0948... ppl: 36.6765  Val Loss: 3.6021\n",
            "Epoch: 9/10... Step: 20032... Loss: 2.7721... ppl: 36.6145  Val Loss: 3.6004\n",
            "Epoch: 9/10... Step: 20064... Loss: 2.4354... ppl: 36.4601  Val Loss: 3.5962\n",
            "Epoch: 9/10... Step: 20096... Loss: 2.5675... ppl: 36.8519  Val Loss: 3.6069\n",
            "Epoch: 9/10... Step: 20128... Loss: 2.8376... ppl: 37.1632  Val Loss: 3.6153\n",
            "Epoch: 9/10... Step: 20160... Loss: 2.8866... ppl: 36.8991  Val Loss: 3.6082\n",
            "Epoch: 9/10... Step: 20192... Loss: 2.2804... ppl: 36.9794  Val Loss: 3.6104\n",
            "Epoch: 9/10... Step: 20224... Loss: 2.4199... ppl: 36.7711  Val Loss: 3.6047\n",
            "Epoch: 9/10... Step: 20256... Loss: 2.2612... ppl: 36.6565  Val Loss: 3.6016\n",
            "Epoch: 9/10... Step: 20288... Loss: 2.8495... ppl: 36.7552  Val Loss: 3.6043\n",
            "Epoch: 9/10... Step: 20320... Loss: 2.5812... ppl: 36.9808  Val Loss: 3.6104\n",
            "Epoch: 9/10... Step: 20352... Loss: 2.3626... ppl: 36.9252  Val Loss: 3.6089\n",
            "Epoch: 9/10... Step: 20384... Loss: 2.7239... ppl: 36.8124  Val Loss: 3.6058\n",
            "Epoch: 9/10... Step: 20416... Loss: 2.5651... ppl: 36.7105  Val Loss: 3.6031\n",
            "Epoch: 9/10... Step: 20448... Loss: 2.5129... ppl: 36.9624  Val Loss: 3.6099\n",
            "Epoch: 9/10... Step: 20480... Loss: 2.4692... ppl: 36.9099  Val Loss: 3.6085\n",
            "Epoch: 9/10... Step: 20512... Loss: 2.7461... ppl: 37.0187  Val Loss: 3.6114\n",
            "Epoch: 9/10... Step: 20544... Loss: 2.5959... ppl: 37.1404  Val Loss: 3.6147\n",
            "Epoch: 9/10... Step: 20576... Loss: 2.5289... ppl: 37.4117  Val Loss: 3.6220\n",
            "Epoch: 9/10... Step: 20608... Loss: 2.4668... ppl: 37.1063  Val Loss: 3.6138\n",
            "Epoch: 9/10... Step: 20640... Loss: 2.3614... ppl: 37.3217  Val Loss: 3.6196\n",
            "Epoch: 9/10... Step: 20672... Loss: 2.7097... ppl: 37.3841  Val Loss: 3.6212\n",
            "Epoch: 9/10... Step: 20704... Loss: 2.7314... ppl: 36.9832  Val Loss: 3.6105\n",
            "Epoch: 9/10... Step: 20736... Loss: 2.5397... ppl: 37.3305  Val Loss: 3.6198\n",
            "Epoch: 9/10... Step: 20768... Loss: 2.8012... ppl: 37.2544  Val Loss: 3.6178\n",
            "Epoch: 9/10... Step: 20800... Loss: 2.7676... ppl: 37.2758  Val Loss: 3.6183\n",
            "Epoch: 9/10... Step: 20832... Loss: 2.8859... ppl: 37.3219  Val Loss: 3.6196\n",
            "Epoch: 9/10... Step: 20864... Loss: 2.3716... ppl: 37.3172  Val Loss: 3.6195\n",
            "Epoch: 9/10... Step: 20896... Loss: 2.7392... ppl: 37.1225  Val Loss: 3.6142\n",
            "Epoch: 9/10... Step: 20928... Loss: 2.4300... ppl: 37.2255  Val Loss: 3.6170\n",
            "Epoch: 9/10... Step: 20960... Loss: 2.6375... ppl: 37.1566  Val Loss: 3.6151\n",
            "Epoch: 9/10... Step: 20992... Loss: 2.3099... ppl: 37.8604  Val Loss: 3.6339\n",
            "Epoch: 9/10... Step: 21024... Loss: 2.3523... ppl: 37.7847  Val Loss: 3.6319\n",
            "Epoch: 9/10... Step: 21056... Loss: 2.6237... ppl: 37.3024  Val Loss: 3.6191\n",
            "Epoch: 10/10... Step: 21088... Loss: 2.7038... ppl: 37.0439  Val Loss: 3.6121\n",
            "Epoch: 10/10... Step: 21120... Loss: 2.4199... ppl: 37.2055  Val Loss: 3.6165\n",
            "Epoch: 10/10... Step: 21152... Loss: 2.6696... ppl: 37.2660  Val Loss: 3.6181\n",
            "Epoch: 10/10... Step: 21184... Loss: 2.7380... ppl: 37.2530  Val Loss: 3.6177\n",
            "Epoch: 10/10... Step: 21216... Loss: 2.1835... ppl: 37.3191  Val Loss: 3.6195\n",
            "Epoch: 10/10... Step: 21248... Loss: 2.5194... ppl: 37.3461  Val Loss: 3.6202\n",
            "Epoch: 10/10... Step: 21280... Loss: 3.1657... ppl: 37.6387  Val Loss: 3.6280\n",
            "Epoch: 10/10... Step: 21312... Loss: 2.6515... ppl: 37.5949  Val Loss: 3.6269\n",
            "Epoch: 10/10... Step: 21344... Loss: 2.5569... ppl: 37.6588  Val Loss: 3.6286\n",
            "Epoch: 10/10... Step: 21376... Loss: 2.4252... ppl: 37.7237  Val Loss: 3.6303\n",
            "Epoch: 10/10... Step: 21408... Loss: 2.6116... ppl: 38.0196  Val Loss: 3.6381\n",
            "Epoch: 10/10... Step: 21440... Loss: 2.6344... ppl: 37.6193  Val Loss: 3.6275\n",
            "Epoch: 10/10... Step: 21472... Loss: 2.5326... ppl: 37.3765  Val Loss: 3.6210\n",
            "Epoch: 10/10... Step: 21504... Loss: 2.5149... ppl: 37.4473  Val Loss: 3.6229\n",
            "Epoch: 10/10... Step: 21536... Loss: 2.4308... ppl: 37.6193  Val Loss: 3.6275\n",
            "Epoch: 10/10... Step: 21568... Loss: 2.6323... ppl: 38.0154  Val Loss: 3.6380\n",
            "Epoch: 10/10... Step: 21600... Loss: 2.4043... ppl: 37.7348  Val Loss: 3.6306\n",
            "Epoch: 10/10... Step: 21632... Loss: 2.4701... ppl: 37.7860  Val Loss: 3.6319\n",
            "Epoch: 10/10... Step: 21664... Loss: 2.6034... ppl: 37.6184  Val Loss: 3.6275\n",
            "Epoch: 10/10... Step: 21696... Loss: 2.9470... ppl: 37.7623  Val Loss: 3.6313\n",
            "Epoch: 10/10... Step: 21728... Loss: 2.4495... ppl: 38.0029  Val Loss: 3.6377\n",
            "Epoch: 10/10... Step: 21760... Loss: 2.1000... ppl: 37.8490  Val Loss: 3.6336\n",
            "Epoch: 10/10... Step: 21792... Loss: 2.3263... ppl: 37.9335  Val Loss: 3.6358\n",
            "Epoch: 10/10... Step: 21824... Loss: 2.4457... ppl: 37.9261  Val Loss: 3.6356\n",
            "Epoch: 10/10... Step: 21856... Loss: 2.6182... ppl: 37.7912  Val Loss: 3.6321\n",
            "Epoch: 10/10... Step: 21888... Loss: 2.2369... ppl: 37.9039  Val Loss: 3.6351\n",
            "Epoch: 10/10... Step: 21920... Loss: 2.9535... ppl: 38.0883  Val Loss: 3.6399\n",
            "Epoch: 10/10... Step: 21952... Loss: 2.7626... ppl: 38.2956  Val Loss: 3.6453\n",
            "Epoch: 10/10... Step: 21984... Loss: 2.0445... ppl: 37.8305  Val Loss: 3.6331\n",
            "Epoch: 10/10... Step: 22016... Loss: 2.6936... ppl: 37.8379  Val Loss: 3.6333\n",
            "Epoch: 10/10... Step: 22048... Loss: 2.1662... ppl: 38.2198  Val Loss: 3.6434\n",
            "Epoch: 10/10... Step: 22080... Loss: 2.7808... ppl: 38.2859  Val Loss: 3.6451\n",
            "Epoch: 10/10... Step: 22112... Loss: 2.0152... ppl: 37.9959  Val Loss: 3.6375\n",
            "Epoch: 10/10... Step: 22144... Loss: 2.3155... ppl: 38.4102  Val Loss: 3.6483\n",
            "Epoch: 10/10... Step: 22176... Loss: 2.2402... ppl: 38.2529  Val Loss: 3.6442\n",
            "Epoch: 10/10... Step: 22208... Loss: 2.9479... ppl: 38.0213  Val Loss: 3.6381\n",
            "Epoch: 10/10... Step: 22240... Loss: 2.4986... ppl: 38.0471  Val Loss: 3.6388\n",
            "Epoch: 10/10... Step: 22272... Loss: 2.6030... ppl: 37.5718  Val Loss: 3.6263\n",
            "Epoch: 10/10... Step: 22304... Loss: 2.7523... ppl: 37.6978  Val Loss: 3.6296\n",
            "Epoch: 10/10... Step: 22336... Loss: 2.4713... ppl: 37.8047  Val Loss: 3.6324\n",
            "Epoch: 10/10... Step: 22368... Loss: 3.0893... ppl: 37.7709  Val Loss: 3.6315\n",
            "Epoch: 10/10... Step: 22400... Loss: 2.4806... ppl: 37.7523  Val Loss: 3.6310\n",
            "Epoch: 10/10... Step: 22432... Loss: 2.4424... ppl: 37.8017  Val Loss: 3.6324\n",
            "Epoch: 10/10... Step: 22464... Loss: 1.7760... ppl: 38.3211  Val Loss: 3.6460\n",
            "Epoch: 10/10... Step: 22496... Loss: 2.3965... ppl: 38.0341  Val Loss: 3.6385\n",
            "Epoch: 10/10... Step: 22528... Loss: 2.9189... ppl: 38.2440  Val Loss: 3.6440\n",
            "Epoch: 10/10... Step: 22560... Loss: 2.5333... ppl: 37.9556  Val Loss: 3.6364\n",
            "Epoch: 10/10... Step: 22592... Loss: 2.3196... ppl: 37.7809  Val Loss: 3.6318\n",
            "Epoch: 10/10... Step: 22624... Loss: 2.9003... ppl: 37.8688  Val Loss: 3.6341\n",
            "Epoch: 10/10... Step: 22656... Loss: 2.3410... ppl: 38.0127  Val Loss: 3.6379\n",
            "Epoch: 10/10... Step: 22688... Loss: 2.9635... ppl: 38.0375  Val Loss: 3.6386\n",
            "Epoch: 10/10... Step: 22720... Loss: 2.5103... ppl: 37.9422  Val Loss: 3.6361\n",
            "Epoch: 10/10... Step: 22752... Loss: 2.8211... ppl: 37.8257  Val Loss: 3.6330\n",
            "Epoch: 10/10... Step: 22784... Loss: 2.6808... ppl: 38.1689  Val Loss: 3.6420\n",
            "Epoch: 10/10... Step: 22816... Loss: 2.5911... ppl: 38.1426  Val Loss: 3.6413\n",
            "Epoch: 10/10... Step: 22848... Loss: 2.7597... ppl: 38.0837  Val Loss: 3.6398\n",
            "Epoch: 10/10... Step: 22880... Loss: 2.1212... ppl: 38.3971  Val Loss: 3.6480\n",
            "Epoch: 10/10... Step: 22912... Loss: 1.9496... ppl: 38.4826  Val Loss: 3.6502\n",
            "Epoch: 10/10... Step: 22944... Loss: 2.8356... ppl: 38.3032  Val Loss: 3.6455\n",
            "Epoch: 10/10... Step: 22976... Loss: 2.4951... ppl: 38.2985  Val Loss: 3.6454\n",
            "Epoch: 10/10... Step: 23008... Loss: 2.6816... ppl: 38.5975  Val Loss: 3.6532\n",
            "Epoch: 10/10... Step: 23040... Loss: 2.3795... ppl: 38.2707  Val Loss: 3.6447\n",
            "Epoch: 10/10... Step: 23072... Loss: 2.5839... ppl: 38.4308  Val Loss: 3.6489\n",
            "Epoch: 10/10... Step: 23104... Loss: 2.5373... ppl: 38.3935  Val Loss: 3.6479\n",
            "Epoch: 10/10... Step: 23136... Loss: 2.4499... ppl: 38.2927  Val Loss: 3.6453\n",
            "Epoch: 10/10... Step: 23168... Loss: 2.4845... ppl: 38.3210  Val Loss: 3.6460\n",
            "Epoch: 10/10... Step: 23200... Loss: 2.2445... ppl: 38.4909  Val Loss: 3.6504\n",
            "Epoch: 10/10... Step: 23232... Loss: 2.4400... ppl: 38.3388  Val Loss: 3.6465\n",
            "Epoch: 10/10... Step: 23264... Loss: 2.5296... ppl: 38.2140  Val Loss: 3.6432\n",
            "Epoch: 10/10... Step: 23296... Loss: 2.5078... ppl: 38.1256  Val Loss: 3.6409\n",
            "Epoch: 10/10... Step: 23328... Loss: 2.4431... ppl: 38.7979  Val Loss: 3.6584\n",
            "Epoch: 10/10... Step: 23360... Loss: 3.0478... ppl: 38.9979  Val Loss: 3.6635\n",
            "Epoch: 10/10... Step: 23392... Loss: 2.6596... ppl: 38.6025  Val Loss: 3.6533\n",
            "Epoch: 10/10... Step: 23424... Loss: 2.0857... ppl: 38.2621  Val Loss: 3.6445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdXbfYcTxcZa",
        "colab_type": "text"
      },
      "source": [
        "# **6. Text Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4TgKxdhxe7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "173220cd-d876-4f62-fa28-cb924231a2ec"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "net.load_state_dict(torch.load(path))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGL5AKibxmMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to generate one token\n",
        "def predict(net, tkn, h=None):\n",
        "         \n",
        "  # tensor inputs\n",
        "  x = np.array([[token2int[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "  \n",
        "  if(torch.cuda.is_available()):\n",
        "      inputs = inputs.cuda()\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  if(torch.cuda.is_available()):\n",
        "      p = p.cpu()\n",
        "\n",
        "  p = p.numpy()\n",
        "  sampled_token_index = np.argmax(p, axis = 1)[0]\n",
        "  \n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int2token[sampled_token_index], h"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2DpfbHBxt7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to fetch generated sequence\n",
        "def sample(net, size = 2, seed_text='it is'):\n",
        "        \n",
        "    if(torch.cuda.is_available()):\n",
        "        net.cuda()\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    toks = seed_text.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in toks:\n",
        "      token, h = predict(net, t, h)\n",
        "    \n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(net, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZQTdb6Nx116",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "dbe4f4c0-848a-4ec5-a753-5cb107de4675"
      },
      "source": [
        "# seed texts\n",
        "seeds = [\"i want to\",\n",
        "         \"how about a cup\",\n",
        "         \"i don't want\",\n",
        "         \"can you send\",\n",
        "         \"my car\"]\n",
        "\n",
        "# number of tokens to generate\n",
        "num_toks = 6\n",
        "\n",
        "# text generation\n",
        "for s in seeds:\n",
        "  # get generated text from the model\n",
        "  text_gen = sample(net, num_toks, seed_text=s)\n",
        "  # print the result\n",
        "  print(\"seed text:\", s, \">> output:\",text_gen)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed text: i want to >> output: i want to order a pizza from pizza hut\n",
            "\n",
            "\n",
            "seed text: how about a cup >> output: how about a cup of caramel mocha with coconut milk\n",
            "\n",
            "\n",
            "seed text: i don't want >> output: i don't want to drive it to the office\n",
            "\n",
            "\n",
            "seed text: can you send >> output: can you send me the confirmation to my phone\n",
            "\n",
            "\n",
            "seed text: my car >> output: my car is making a weird noise when\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}